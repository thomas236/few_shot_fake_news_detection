{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b64fb204",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b64fb204",
        "outputId": "575486cd-e160-4f2b-f4f3-3077eb895434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-01 11:27:52--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-08-01 11:27:52--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-08-01 11:27:53--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.97MB/s    in 2m 43s  \n",
            "\n",
            "2023-08-01 11:30:37 (5.06 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Fake News       1.00      0.05      0.10        19\n",
            "   Real News       0.53      1.00      0.69        20\n",
            "\n",
            "    accuracy                           0.54        39\n",
            "   macro avg       0.76      0.53      0.39        39\n",
            "weighted avg       0.76      0.54      0.40        39\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import importlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if NLTK is installed, if not install it\n",
        "package = \"nltk\"\n",
        "try:\n",
        "    importlib.import_module(package)\n",
        "except ImportError:\n",
        "    !pip install {package}\n",
        "    importlib.import_module(package)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Check if GloVe embeddings are present, if not download them\n",
        "if not os.path.isfile('./glove.6B.300d.txt'):\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip glove*.zip\n",
        "\n",
        "def load_glove(path):\n",
        "    \"\"\"Loads GloVe embeddings.\"\"\"\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "\n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    m = X.shape[0]  # number of training examples\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "\n",
        "    for i in range(m):\n",
        "        sentence_words = X[i].lower().split()\n",
        "        j = 0\n",
        "\n",
        "        for w in sentence_words:\n",
        "            if j >= max_len:\n",
        "                break\n",
        "            if w in word_to_index:\n",
        "                X_indices[i, j] = word_to_index[w]\n",
        "                j = j + 1\n",
        "    return X_indices\n",
        "\n",
        "# Load the GloVe embeddings\n",
        "word_to_index, index_to_word, word_to_vec_map = load_glove('./glove.6B.300d.txt')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/news_200.csv')\n",
        "\n",
        "# Preprocess the dataset\n",
        "df = df.dropna(subset=['text', 'title'])\n",
        "\n",
        "# Tokenize the text\n",
        "max_len = 256  # choose a max length\n",
        "tokens = sentences_to_indices(df['text'].values, word_to_index, max_len)\n",
        "\n",
        "# Define the model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index):\n",
        "        super(CNN, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(self.pretrained_embedding(word_to_vec_map, word_to_index, vocab_size, embedding_size))\n",
        "        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3)\n",
        "        self.fc = nn.Linear(256 * (max_len - 4), num_classes)\n",
        "\n",
        "    def pretrained_embedding(self, word_to_vec_map, word_to_index, vocab_size, embedding_size):\n",
        "        matrix_len = vocab_size\n",
        "        weights_matrix = np.zeros((matrix_len, embedding_size))\n",
        "        words_found = 0\n",
        "        for word, i in word_to_index.items():\n",
        "            try:\n",
        "                weights_matrix[i] = word_to_vec_map[word]\n",
        "                words_found += 1\n",
        "            except KeyError:\n",
        "                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size, ))\n",
        "        return torch.from_numpy(weights_matrix).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.long())\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vocab_size = len(word_to_index) + 1\n",
        "embedding_size = 300  # adjust the embedding size as needed\n",
        "num_classes = 2\n",
        "model = CNN(vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index)\n",
        "model = model.to(device)\n",
        "\n",
        "# Convert labels into torch tensors\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
        "    tokens, df['label'].values, random_state=100, test_size=0.2)\n",
        "\n",
        "# Convert into torch tensors\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Create DataLoader for the training set\n",
        "train_data = TensorDataset(train_inputs, train_labels)\n",
        "train_dataloader = DataLoader(train_data, batch_size=32)\n",
        "\n",
        "# Training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(1):\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(b_input_ids)\n",
        "        loss = criterion(outputs, b_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "# Create DataLoader for the test set\n",
        "test_data = TensorDataset(test_inputs, test_labels)\n",
        "test_dataloader = DataLoader(test_data, batch_size=32)\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = [b.to(device) for b in batch]\n",
        "    b_input_ids, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = outputs.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# For each input batch, pick the label (0 or 1) with the higher score\n",
        "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ms77gpR_Cc05"
      },
      "id": "Ms77gpR_Cc05"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import importlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if NLTK is installed, if not install it\n",
        "package = \"nltk\"\n",
        "try:\n",
        "    importlib.import_module(package)\n",
        "except ImportError:\n",
        "    !pip install {package}\n",
        "    importlib.import_module(package)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Check if GloVe embeddings are present, if not download them\n",
        "if not os.path.isfile('./glove.6B.300d.txt'):\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip glove*.zip\n",
        "\n",
        "def load_glove(path):\n",
        "    \"\"\"Loads GloVe embeddings.\"\"\"\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "\n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    m = X.shape[0]  # number of training examples\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "\n",
        "    for i in range(m):\n",
        "        sentence_words = X[i].lower().split()\n",
        "        j = 0\n",
        "\n",
        "        for w in sentence_words:\n",
        "            if j >= max_len:\n",
        "                break\n",
        "            if w in word_to_index:\n",
        "                X_indices[i, j] = word_to_index[w]\n",
        "                j = j + 1\n",
        "    return X_indices\n",
        "\n",
        "# Load the GloVe embeddings\n",
        "word_to_index, index_to_word, word_to_vec_map = load_glove('./glove.6B.300d.txt')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/news_500.csv')\n",
        "\n",
        "# Preprocess the dataset\n",
        "df = df.dropna(subset=['text', 'title'])\n",
        "\n",
        "# Tokenize the text\n",
        "max_len = 256  # choose a max length\n",
        "tokens = sentences_to_indices(df['text'].values, word_to_index, max_len)\n",
        "\n",
        "# Define the model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index):\n",
        "        super(CNN, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(self.pretrained_embedding(word_to_vec_map, word_to_index, vocab_size, embedding_size))\n",
        "        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3)\n",
        "        self.fc = nn.Linear(256 * (max_len - 4), num_classes)\n",
        "\n",
        "    def pretrained_embedding(self, word_to_vec_map, word_to_index, vocab_size, embedding_size):\n",
        "        matrix_len = vocab_size\n",
        "        weights_matrix = np.zeros((matrix_len, embedding_size))\n",
        "        words_found = 0\n",
        "        for word, i in word_to_index.items():\n",
        "            try:\n",
        "                weights_matrix[i] = word_to_vec_map[word]\n",
        "                words_found += 1\n",
        "            except KeyError:\n",
        "                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size, ))\n",
        "        return torch.from_numpy(weights_matrix).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.long())\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vocab_size = len(word_to_index) + 1\n",
        "embedding_size = 300  # adjust the embedding size as needed\n",
        "num_classes = 2\n",
        "model = CNN(vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index)\n",
        "model = model.to(device)\n",
        "\n",
        "# Convert labels into torch tensors\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
        "    tokens, df['label'].values, random_state=100, test_size=0.2)\n",
        "\n",
        "# Convert into torch tensors\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Create DataLoader for the training set\n",
        "train_data = TensorDataset(train_inputs, train_labels)\n",
        "train_dataloader = DataLoader(train_data, batch_size=32)\n",
        "\n",
        "# Training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(1):\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(b_input_ids)\n",
        "        loss = criterion(outputs, b_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "# Create DataLoader for the test set\n",
        "test_data = TensorDataset(test_inputs, test_labels)\n",
        "test_dataloader = DataLoader(test_data, batch_size=32)\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = [b.to(device) for b in batch]\n",
        "    b_input_ids, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = outputs.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# For each input batch, pick the label (0 or 1) with the higher score\n",
        "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33ZAflvm-t8X",
        "outputId": "8f332d46-4598-4eff-eba4-2aa9388d7c39"
      },
      "id": "33ZAflvm-t8X",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Fake News       0.61      0.94      0.74        54\n",
            "   Real News       0.80      0.27      0.40        45\n",
            "\n",
            "    accuracy                           0.64        99\n",
            "   macro avg       0.70      0.61      0.57        99\n",
            "weighted avg       0.69      0.64      0.58        99\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import importlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if NLTK is installed, if not install it\n",
        "package = \"nltk\"\n",
        "try:\n",
        "    importlib.import_module(package)\n",
        "except ImportError:\n",
        "    !pip install {package}\n",
        "    importlib.import_module(package)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Check if GloVe embeddings are present, if not download them\n",
        "if not os.path.isfile('./glove.6B.300d.txt'):\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip glove*.zip\n",
        "\n",
        "def load_glove(path):\n",
        "    \"\"\"Loads GloVe embeddings.\"\"\"\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "\n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    m = X.shape[0]  # number of training examples\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "\n",
        "    for i in range(m):\n",
        "        sentence_words = X[i].lower().split()\n",
        "        j = 0\n",
        "\n",
        "        for w in sentence_words:\n",
        "            if j >= max_len:\n",
        "                break\n",
        "            if w in word_to_index:\n",
        "                X_indices[i, j] = word_to_index[w]\n",
        "                j = j + 1\n",
        "    return X_indices\n",
        "\n",
        "# Load the GloVe embeddings\n",
        "word_to_index, index_to_word, word_to_vec_map = load_glove('./glove.6B.300d.txt')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/news_10000.csv')\n",
        "\n",
        "# Preprocess the dataset\n",
        "df = df.dropna(subset=['text', 'title'])\n",
        "\n",
        "# Tokenize the text\n",
        "max_len = 256  # choose a max length\n",
        "tokens = sentences_to_indices(df['text'].values, word_to_index, max_len)\n",
        "\n",
        "# Define the model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index):\n",
        "        super(CNN, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(self.pretrained_embedding(word_to_vec_map, word_to_index, vocab_size, embedding_size))\n",
        "        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3)\n",
        "        self.fc = nn.Linear(256 * (max_len - 4), num_classes)\n",
        "\n",
        "    def pretrained_embedding(self, word_to_vec_map, word_to_index, vocab_size, embedding_size):\n",
        "        matrix_len = vocab_size\n",
        "        weights_matrix = np.zeros((matrix_len, embedding_size))\n",
        "        words_found = 0\n",
        "        for word, i in word_to_index.items():\n",
        "            try:\n",
        "                weights_matrix[i] = word_to_vec_map[word]\n",
        "                words_found += 1\n",
        "            except KeyError:\n",
        "                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size, ))\n",
        "        return torch.from_numpy(weights_matrix).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.long())\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vocab_size = len(word_to_index) + 1\n",
        "embedding_size = 300  # adjust the embedding size as needed\n",
        "num_classes = 2\n",
        "model = CNN(vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index)\n",
        "model = model.to(device)\n",
        "\n",
        "# Convert labels into torch tensors\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
        "    tokens, df['label'].values, random_state=100, test_size=0.2)\n",
        "\n",
        "# Convert into torch tensors\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Create DataLoader for the training set\n",
        "train_data = TensorDataset(train_inputs, train_labels)\n",
        "train_dataloader = DataLoader(train_data, batch_size=32)\n",
        "\n",
        "# Training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(1):\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(b_input_ids)\n",
        "        loss = criterion(outputs, b_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "# Create DataLoader for the test set\n",
        "test_data = TensorDataset(test_inputs, test_labels)\n",
        "test_dataloader = DataLoader(test_data, batch_size=32)\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = [b.to(device) for b in batch]\n",
        "    b_input_ids, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = outputs.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# For each input batch, pick the label (0 or 1) with the higher score\n",
        "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOCcCfOK-vsS",
        "outputId": "92a10218-67fe-47a4-ecf7-251981779256"
      },
      "id": "eOCcCfOK-vsS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Fake News       0.93      0.93      0.93      1029\n",
            "   Real News       0.93      0.93      0.93       912\n",
            "\n",
            "    accuracy                           0.93      1941\n",
            "   macro avg       0.93      0.93      0.93      1941\n",
            "weighted avg       0.93      0.93      0.93      1941\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import importlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if NLTK is installed, if not install it\n",
        "package = \"nltk\"\n",
        "try:\n",
        "    importlib.import_module(package)\n",
        "except ImportError:\n",
        "    !pip install {package}\n",
        "    importlib.import_module(package)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Check if GloVe embeddings are present, if not download them\n",
        "if not os.path.isfile('./glove.6B.300d.txt'):\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip glove*.zip\n",
        "\n",
        "def load_glove(path):\n",
        "    \"\"\"Loads GloVe embeddings.\"\"\"\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "\n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    m = X.shape[0]  # number of training examples\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "\n",
        "    for i in range(m):\n",
        "        sentence_words = X[i].lower().split()\n",
        "        j = 0\n",
        "\n",
        "        for w in sentence_words:\n",
        "            if j >= max_len:\n",
        "                break\n",
        "            if w in word_to_index:\n",
        "                X_indices[i, j] = word_to_index[w]\n",
        "                j = j + 1\n",
        "    return X_indices\n",
        "\n",
        "# Load the GloVe embeddings\n",
        "word_to_index, index_to_word, word_to_vec_map = load_glove('./glove.6B.300d.txt')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv')\n",
        "\n",
        "# Preprocess the dataset\n",
        "df = df.dropna(subset=['text', 'title'])\n",
        "\n",
        "# Tokenize the text\n",
        "max_len = 256  # choose a max length\n",
        "tokens = sentences_to_indices(df['text'].values, word_to_index, max_len)\n",
        "\n",
        "# Define the model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index):\n",
        "        super(CNN, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(self.pretrained_embedding(word_to_vec_map, word_to_index, vocab_size, embedding_size))\n",
        "        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3)\n",
        "        self.fc = nn.Linear(256 * (max_len - 4), num_classes)\n",
        "\n",
        "    def pretrained_embedding(self, word_to_vec_map, word_to_index, vocab_size, embedding_size):\n",
        "        matrix_len = vocab_size\n",
        "        weights_matrix = np.zeros((matrix_len, embedding_size))\n",
        "        words_found = 0\n",
        "        for word, i in word_to_index.items():\n",
        "            try:\n",
        "                weights_matrix[i] = word_to_vec_map[word]\n",
        "                words_found += 1\n",
        "            except KeyError:\n",
        "                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size, ))\n",
        "        return torch.from_numpy(weights_matrix).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.long())\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vocab_size = len(word_to_index) + 1\n",
        "embedding_size = 300  # adjust the embedding size as needed\n",
        "num_classes = 2\n",
        "model = CNN(vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index)\n",
        "model = model.to(device)\n",
        "\n",
        "# Convert labels into torch tensors\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
        "    tokens, df['label'].values, random_state=100, test_size=0.2)\n",
        "\n",
        "# Convert into torch tensors\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Create DataLoader for the training set\n",
        "train_data = TensorDataset(train_inputs, train_labels)\n",
        "train_dataloader = DataLoader(train_data, batch_size=32)\n",
        "\n",
        "# Training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(1):\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(b_input_ids)\n",
        "        loss = criterion(outputs, b_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "# Create DataLoader for the test set\n",
        "test_data = TensorDataset(test_inputs, test_labels)\n",
        "test_dataloader = DataLoader(test_data, batch_size=32)\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = [b.to(device) for b in batch]\n",
        "    b_input_ids, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = outputs.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# For each input batch, pick the label (0 or 1) with the higher score\n",
        "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ-uZEFz-xKA",
        "outputId": "0d1b15e0-d516-43d1-8a4c-c7942d5c8f33"
      },
      "id": "JJ-uZEFz-xKA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Fake News       0.94      0.98      0.96      2104\n",
            "   Real News       0.98      0.93      0.96      1937\n",
            "\n",
            "    accuracy                           0.96      4041\n",
            "   macro avg       0.96      0.96      0.96      4041\n",
            "weighted avg       0.96      0.96      0.96      4041\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}