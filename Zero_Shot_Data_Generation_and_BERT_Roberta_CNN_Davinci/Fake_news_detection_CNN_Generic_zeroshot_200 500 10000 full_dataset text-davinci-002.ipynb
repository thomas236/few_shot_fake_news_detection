{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"b64fb204"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"name":"stdout","output_type":"stream","text":["--2023-08-19 18:35:30--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2023-08-19 18:35:30--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2023-08-19 18:35:31--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================\u003e] 822.24M  5.02MB/s    in 2m 38s  \n","\n","2023-08-19 18:38:10 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n","Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n","              precision    recall  f1-score   support\n","\n","   Fake News       0.58      1.00      0.73        19\n","   Real News       1.00      0.30      0.46        20\n","\n","    accuracy                           0.64        39\n","   macro avg       0.79      0.65      0.60        39\n","weighted avg       0.79      0.64      0.59        39\n","\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","import importlib\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","import os\n","from collections import Counter\n","from tqdm import tqdm\n","\n","# Check if NLTK is installed, if not install it\n","package = \"nltk\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","import nltk\n","nltk.download('punkt')\n","\n","# Check if GloVe embeddings are present, if not download them\n","if not os.path.isfile('./glove.6B.300d.txt'):\n","    !wget http://nlp.stanford.edu/data/glove.6B.zip\n","    !unzip glove*.zip\n","\n","def load_glove(path):\n","    \"\"\"Loads GloVe embeddings.\"\"\"\n","    with open(path, 'r', encoding='utf-8') as f:\n","        words = set()\n","        word_to_vec_map = {}\n","        for line in f:\n","            line = line.strip().split()\n","            curr_word = line[0]\n","            words.add(curr_word)\n","            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n","\n","        i = 1\n","        words_to_index = {}\n","        index_to_words = {}\n","        for w in sorted(words):\n","            words_to_index[w] = i\n","            index_to_words[i] = w\n","            i = i + 1\n","    return words_to_index, index_to_words, word_to_vec_map\n","\n","def sentences_to_indices(X, word_to_index, max_len):\n","    m = X.shape[0]  # number of training examples\n","    X_indices = np.zeros((m, max_len))\n","\n","    for i in range(m):\n","        sentence_words = X[i].lower().split()\n","        j = 0\n","\n","        for w in sentence_words:\n","            if j \u003e= max_len:\n","                break\n","            if w in word_to_index:\n","                X_indices[i, j] = word_to_index[w]\n","                j = j + 1\n","    return X_indices\n","\n","# Load the GloVe embeddings\n","word_to_index, index_to_word, word_to_vec_map = load_glove('./glove.6B.300d.txt')\n","\n","# Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/news_200.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/generated_combined_data200text-davinci-002.csv\")\n","\n","\n","\n","# Preprocess the dataset\n","df = df.dropna(subset=['text', 'title'])\n","df1 = df1.dropna(subset=['text', 'title'])\n","\n","# Tokenize the text\n","max_len = 256  # choose a max length\n","tokens = sentences_to_indices(df['text'].values, word_to_index, max_len)\n","tokens1 = sentences_to_indices(df1['text'].values, word_to_index, max_len)\n","\n","# Define the model\n","class CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index):\n","        super(CNN, self).__init__()\n","        self.embedding = nn.Embedding.from_pretrained(self.pretrained_embedding(word_to_vec_map, word_to_index, vocab_size, embedding_size))\n","        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=3)\n","        self.conv2 = nn.Conv1d(128, 256, kernel_size=3)\n","        self.fc = nn.Linear(256 * (max_len - 4), num_classes)\n","\n","    def pretrained_embedding(self, word_to_vec_map, word_to_index, vocab_size, embedding_size):\n","        matrix_len = vocab_size\n","        weights_matrix = np.zeros((matrix_len, embedding_size))\n","        words_found = 0\n","        for word, i in word_to_index.items():\n","            try:\n","                weights_matrix[i] = word_to_vec_map[word]\n","                words_found += 1\n","            except KeyError:\n","                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size, ))\n","        return torch.from_numpy(weights_matrix).float()\n","\n","    def forward(self, x):\n","        x = self.embedding(x.long())\n","        x = x.permute(0, 2, 1)\n","        x = self.conv1(x)\n","        x = nn.functional.relu(x)\n","        x = self.conv2(x)\n","        x = nn.functional.relu(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","# Create the model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","vocab_size = len(word_to_index) + 1\n","embedding_size = 300  # adjust the embedding size as needed\n","num_classes = 2\n","model = CNN(vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index)\n","model = model.to(device)\n","\n","# Convert labels into torch tensors\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n","    tokens, df['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(\n","    tokens1, df1['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs,train_labels=train_inputs1,train_labels1\n","\n","# Convert into torch tensors\n","train_inputs = torch.tensor(train_inputs)\n","test_inputs = torch.tensor(test_inputs)\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","\n","# Create DataLoader for the training set\n","train_data = TensorDataset(train_inputs, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=32)\n","\n","# Training\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(1):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(b_input_ids)\n","        loss = criterion(outputs, b_labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","# Evaluation\n","# Create DataLoader for the test set\n","test_data = TensorDataset(test_inputs, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=32)\n","\n","model.eval()\n","predictions = []\n","true_labels = []\n","\n","for batch in test_dataloader:\n","    batch = [b.to(device) for b in batch]\n","    b_input_ids, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","\n","    # Move logits and labels to CPU\n","    logits = outputs.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# For each input batch, pick the label (0 or 1) with the higher score\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","\n","# Print the classification report\n","print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CA_YEjAmUODK"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","import importlib\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","import os\n","from collections import Counter\n","from tqdm import tqdm\n","\n","# Check if NLTK is installed, if not install it\n","package = \"nltk\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","import nltk\n","nltk.download('punkt')\n","\n","# Check if GloVe embeddings are present, if not download them\n","if not os.path.isfile('./glove.6B.300d.txt'):\n","    !wget http://nlp.stanford.edu/data/glove.6B.zip\n","    !unzip glove*.zip\n","\n","def load_glove(path):\n","    \"\"\"Loads GloVe embeddings.\"\"\"\n","    with open(path, 'r', encoding='utf-8') as f:\n","        words = set()\n","        word_to_vec_map = {}\n","        for line in f:\n","            line = line.strip().split()\n","            curr_word = line[0]\n","            words.add(curr_word)\n","            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n","\n","        i = 1\n","        words_to_index = {}\n","        index_to_words = {}\n","        for w in sorted(words):\n","            words_to_index[w] = i\n","            index_to_words[i] = w\n","            i = i + 1\n","    return words_to_index, index_to_words, word_to_vec_map\n","\n","def sentences_to_indices(X, word_to_index, max_len):\n","    m = X.shape[0]  # number of training examples\n","    X_indices = np.zeros((m, max_len))\n","\n","    for i in range(m):\n","        sentence_words = X[i].lower().split()\n","        j = 0\n","\n","        for w in sentence_words:\n","            if j \u003e= max_len:\n","                break\n","            if w in word_to_index:\n","                X_indices[i, j] = word_to_index[w]\n","                j = j + 1\n","    return X_indices\n","\n","# Load the GloVe embeddings\n","word_to_index, index_to_word, word_to_vec_map = load_glove('./glove.6B.300d.txt')\n","\n","# Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/news_500.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/generated_combined_data500text-davinci-002.csv\")\n","\n","\n","# Preprocess the dataset\n","df = df.dropna(subset=['text', 'title'])\n","df1 = df1.dropna(subset=['text', 'title'])\n","\n","# Tokenize the text\n","max_len = 256  # choose a max length\n","tokens = sentences_to_indices(df['text'].values, word_to_index, max_len)\n","tokens1 = sentences_to_indices(df1['text'].values, word_to_index, max_len)\n","\n","# Define the model\n","class CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index):\n","        super(CNN, self).__init__()\n","        self.embedding = nn.Embedding.from_pretrained(self.pretrained_embedding(word_to_vec_map, word_to_index, vocab_size, embedding_size))\n","        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=3)\n","        self.conv2 = nn.Conv1d(128, 256, kernel_size=3)\n","        self.fc = nn.Linear(256 * (max_len - 4), num_classes)\n","\n","    def pretrained_embedding(self, word_to_vec_map, word_to_index, vocab_size, embedding_size):\n","        matrix_len = vocab_size\n","        weights_matrix = np.zeros((matrix_len, embedding_size))\n","        words_found = 0\n","        for word, i in word_to_index.items():\n","            try:\n","                weights_matrix[i] = word_to_vec_map[word]\n","                words_found += 1\n","            except KeyError:\n","                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size, ))\n","        return torch.from_numpy(weights_matrix).float()\n","\n","    def forward(self, x):\n","        x = self.embedding(x.long())\n","        x = x.permute(0, 2, 1)\n","        x = self.conv1(x)\n","        x = nn.functional.relu(x)\n","        x = self.conv2(x)\n","        x = nn.functional.relu(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","# Create the model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","vocab_size = len(word_to_index) + 1\n","embedding_size = 300  # adjust the embedding size as needed\n","num_classes = 2\n","model = CNN(vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index)\n","model = model.to(device)\n","\n","# Convert labels into torch tensors\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n","    tokens, df['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(\n","    tokens1, df1['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs,train_labels=train_inputs1,train_labels1\n","\n","# Convert into torch tensors\n","train_inputs = torch.tensor(train_inputs)\n","test_inputs = torch.tensor(test_inputs)\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","\n","# Create DataLoader for the training set\n","train_data = TensorDataset(train_inputs, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=32)\n","\n","# Training\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(1):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(b_input_ids)\n","        loss = criterion(outputs, b_labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","# Evaluation\n","# Create DataLoader for the test set\n","test_data = TensorDataset(test_inputs, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=32)\n","\n","model.eval()\n","predictions = []\n","true_labels = []\n","\n","for batch in test_dataloader:\n","    batch = [b.to(device) for b in batch]\n","    b_input_ids, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","\n","    # Move logits and labels to CPU\n","    logits = outputs.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# For each input batch, pick the label (0 or 1) with the higher score\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","\n","# Print the classification report\n","print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVFWJ6p4US63"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","import importlib\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","import os\n","from collections import Counter\n","from tqdm import tqdm\n","\n","# Check if NLTK is installed, if not install it\n","package = \"nltk\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","import nltk\n","nltk.download('punkt')\n","\n","# Check if GloVe embeddings are present, if not download them\n","if not os.path.isfile('./glove.6B.300d.txt'):\n","    !wget http://nlp.stanford.edu/data/glove.6B.zip\n","    !unzip glove*.zip\n","\n","def load_glove(path):\n","    \"\"\"Loads GloVe embeddings.\"\"\"\n","    with open(path, 'r', encoding='utf-8') as f:\n","        words = set()\n","        word_to_vec_map = {}\n","        for line in f:\n","            line = line.strip().split()\n","            curr_word = line[0]\n","            words.add(curr_word)\n","            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n","\n","        i = 1\n","        words_to_index = {}\n","        index_to_words = {}\n","        for w in sorted(words):\n","            words_to_index[w] = i\n","            index_to_words[i] = w\n","            i = i + 1\n","    return words_to_index, index_to_words, word_to_vec_map\n","\n","def sentences_to_indices(X, word_to_index, max_len):\n","    m = X.shape[0]  # number of training examples\n","    X_indices = np.zeros((m, max_len))\n","\n","    for i in range(m):\n","        sentence_words = X[i].lower().split()\n","        j = 0\n","\n","        for w in sentence_words:\n","            if j \u003e= max_len:\n","                break\n","            if w in word_to_index:\n","                X_indices[i, j] = word_to_index[w]\n","                j = j + 1\n","    return X_indices\n","\n","# Load the GloVe embeddings\n","word_to_index, index_to_word, word_to_vec_map = load_glove('./glove.6B.300d.txt')\n","\n","# Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/news_10000.csv')\n","df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/train_davinci_10000.csv')\n","\n","\n","# Preprocess the dataset\n","df = df.dropna(subset=['text', 'title'])\n","df1 = df1.dropna(subset=['text', 'title'])\n","\n","# Tokenize the text\n","max_len = 256  # choose a max length\n","tokens = sentences_to_indices(df['text'].values, word_to_index, max_len)\n","tokens1 = sentences_to_indices(df1['text'].values, word_to_index, max_len)\n","\n","# Define the model\n","class CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index):\n","        super(CNN, self).__init__()\n","        self.embedding = nn.Embedding.from_pretrained(self.pretrained_embedding(word_to_vec_map, word_to_index, vocab_size, embedding_size))\n","        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=3)\n","        self.conv2 = nn.Conv1d(128, 256, kernel_size=3)\n","        self.fc = nn.Linear(256 * (max_len - 4), num_classes)\n","\n","    def pretrained_embedding(self, word_to_vec_map, word_to_index, vocab_size, embedding_size):\n","        matrix_len = vocab_size\n","        weights_matrix = np.zeros((matrix_len, embedding_size))\n","        words_found = 0\n","        for word, i in word_to_index.items():\n","            try:\n","                weights_matrix[i] = word_to_vec_map[word]\n","                words_found += 1\n","            except KeyError:\n","                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size, ))\n","        return torch.from_numpy(weights_matrix).float()\n","\n","    def forward(self, x):\n","        x = self.embedding(x.long())\n","        x = x.permute(0, 2, 1)\n","        x = self.conv1(x)\n","        x = nn.functional.relu(x)\n","        x = self.conv2(x)\n","        x = nn.functional.relu(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","# Create the model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","vocab_size = len(word_to_index) + 1\n","embedding_size = 300  # adjust the embedding size as needed\n","num_classes = 2\n","model = CNN(vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index)\n","model = model.to(device)\n","\n","# Convert labels into torch tensors\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n","    tokens, df['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(\n","    tokens1, df1['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs,train_labels=train_inputs1,train_labels1\n","\n","# Convert into torch tensors\n","train_inputs = torch.tensor(train_inputs)\n","test_inputs = torch.tensor(test_inputs)\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","\n","# Create DataLoader for the training set\n","train_data = TensorDataset(train_inputs, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=32)\n","\n","# Training\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(1):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(b_input_ids)\n","        loss = criterion(outputs, b_labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","# Evaluation\n","# Create DataLoader for the test set\n","test_data = TensorDataset(test_inputs, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=32)\n","\n","model.eval()\n","predictions = []\n","true_labels = []\n","\n","for batch in test_dataloader:\n","    batch = [b.to(device) for b in batch]\n","    b_input_ids, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","\n","    # Move logits and labels to CPU\n","    logits = outputs.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# For each input batch, pick the label (0 or 1) with the higher score\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","\n","# Print the classification report\n","print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fzISvqZUfDR"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","import importlib\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","import os\n","from collections import Counter\n","from tqdm import tqdm\n","\n","# Check if NLTK is installed, if not install it\n","package = \"nltk\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","import nltk\n","nltk.download('punkt')\n","\n","# Check if GloVe embeddings are present, if not download them\n","if not os.path.isfile('./glove.6B.300d.txt'):\n","    !wget http://nlp.stanford.edu/data/glove.6B.zip\n","    !unzip glove*.zip\n","\n","def load_glove(path):\n","    \"\"\"Loads GloVe embeddings.\"\"\"\n","    with open(path, 'r', encoding='utf-8') as f:\n","        words = set()\n","        word_to_vec_map = {}\n","        for line in f:\n","            line = line.strip().split()\n","            curr_word = line[0]\n","            words.add(curr_word)\n","            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n","\n","        i = 1\n","        words_to_index = {}\n","        index_to_words = {}\n","        for w in sorted(words):\n","            words_to_index[w] = i\n","            index_to_words[i] = w\n","            i = i + 1\n","    return words_to_index, index_to_words, word_to_vec_map\n","\n","def sentences_to_indices(X, word_to_index, max_len):\n","    m = X.shape[0]  # number of training examples\n","    X_indices = np.zeros((m, max_len))\n","\n","    for i in range(m):\n","        sentence_words = X[i].lower().split()\n","        j = 0\n","\n","        for w in sentence_words:\n","            if j \u003e= max_len:\n","                break\n","            if w in word_to_index:\n","                X_indices[i, j] = word_to_index[w]\n","                j = j + 1\n","    return X_indices\n","\n","# Load the GloVe embeddings\n","word_to_index, index_to_word, word_to_vec_map = load_glove('./glove.6B.300d.txt')\n","\n","# Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/train.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/train_davinci_20400.csv\")\n","\n","\n","# Preprocess the dataset\n","df = df.dropna(subset=['text', 'title'])\n","df1 = df1.dropna(subset=['text', 'title'])\n","\n","# Tokenize the text\n","max_len = 256  # choose a max length\n","tokens = sentences_to_indices(df['text'].values, word_to_index, max_len)\n","tokens1 = sentences_to_indices(df1['text'].values, word_to_index, max_len)\n","\n","# Define the model\n","class CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index):\n","        super(CNN, self).__init__()\n","        self.embedding = nn.Embedding.from_pretrained(self.pretrained_embedding(word_to_vec_map, word_to_index, vocab_size, embedding_size))\n","        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=3)\n","        self.conv2 = nn.Conv1d(128, 256, kernel_size=3)\n","        self.fc = nn.Linear(256 * (max_len - 4), num_classes)\n","\n","    def pretrained_embedding(self, word_to_vec_map, word_to_index, vocab_size, embedding_size):\n","        matrix_len = vocab_size\n","        weights_matrix = np.zeros((matrix_len, embedding_size))\n","        words_found = 0\n","        for word, i in word_to_index.items():\n","            try:\n","                weights_matrix[i] = word_to_vec_map[word]\n","                words_found += 1\n","            except KeyError:\n","                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size, ))\n","        return torch.from_numpy(weights_matrix).float()\n","\n","    def forward(self, x):\n","        x = self.embedding(x.long())\n","        x = x.permute(0, 2, 1)\n","        x = self.conv1(x)\n","        x = nn.functional.relu(x)\n","        x = self.conv2(x)\n","        x = nn.functional.relu(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","# Create the model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","vocab_size = len(word_to_index) + 1\n","embedding_size = 300  # adjust the embedding size as needed\n","num_classes = 2\n","model = CNN(vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index)\n","model = model.to(device)\n","\n","# Convert labels into torch tensors\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n","    tokens, df['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(\n","    tokens1, df1['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs,train_labels=train_inputs1,train_labels1\n","\n","# Convert into torch tensors\n","train_inputs = torch.tensor(train_inputs)\n","test_inputs = torch.tensor(test_inputs)\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","\n","# Create DataLoader for the training set\n","train_data = TensorDataset(train_inputs, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=32)\n","\n","# Training\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(1):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(b_input_ids)\n","        loss = criterion(outputs, b_labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","# Evaluation\n","# Create DataLoader for the test set\n","test_data = TensorDataset(test_inputs, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=32)\n","\n","model.eval()\n","predictions = []\n","true_labels = []\n","\n","for batch in test_dataloader:\n","    batch = [b.to(device) for b in batch]\n","    b_input_ids, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","\n","    # Move logits and labels to CPU\n","    logits = outputs.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# For each input batch, pick the label (0 or 1) with the higher score\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","\n","# Print the classification report\n","print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"]},{"cell_type":"markdown","metadata":{"id":"32da13e4"},"source":["# BERT Section Below"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"","provenance":[{"file_id":"1vZBUP5wBNir1ghAqQRwtucix9xPDJVLm","timestamp":1690888919585},{"file_id":"1txr0rSCHH1cxKaqzxHP3Q2g2lyXcrNtO","timestamp":1690863605933},{"file_id":"1fu7oluUqVxR61mdFxiWL91-hXySJP_sT","timestamp":1690862466976},{"file_id":"1xovG9wwGJp1U2_a3wEk7ts4ie_x9fNqA","timestamp":1686508547571}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":5}