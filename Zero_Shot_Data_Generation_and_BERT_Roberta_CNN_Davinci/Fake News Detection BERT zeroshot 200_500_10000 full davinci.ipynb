{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d7f60b54f6494ad28aeeaf92fc253968","346e69ab6ba14ec18e71cc415a8fd982","1e9084bdfee84b36bc3e051dcd29b6aa","0158b2d45fce45d5af5c6eaae13790ab","a36bb37d18b24099a14303b8eed565e7","61feb8abe662418e9bf5d65be6f95a0c","3dd383bddfad4d1e840609b15b859d99","9a2df728a8b54f13a437642726e6d5a7","d0969066a3a6441fb150b606b473af37","438483f07eae4b18817335557db4d078","238c1c773499433796df4954e4b16af4","56980cf7f0334d8ca730d45b8a2517d2","14de9d6de8f24eddac2133e22a7e4411","e086f878c508494d933dcfbb8d345b27","9c8da210af0048e786e352c9e4d9638b","eb6fd7f5bfb3466ab2fb59db7f7ac317","ded24a81058944dfa36e5b3bc10896cd","828d3c399ba14dbc9b37606c193dc4b5","f9714662d14e4142ac05a9a34f289f9d","6de826524e65489a8c3435a661c43d40","eac8aa8ed6ce454eae24b708ee8b4e7e","9a0eafee9266489996ea31c6e18ae8b7","1bae2276258d49caa2a8b496c503abf7","6147f310184f4dc2b4aedfcc1758ad76","f940317571df42519017da214bc2b813","e1308300d83c4666ac36b16a1266d888","82412c38c46a47839cdbec61835ecc59","1f360bc4994e4d2ea8d555c5618797be","83f994d19f504c568f9d96e6ab5f5ee0","dabd4c6fa9de422187f6732d1337bba1","2f46abf0835a40ea9b621a038ba3d00e","cdab23fb702c4c36bad607fd78e492c7","5357cf03e2fc45268dde6cda296e61f2","4ea5618845fe49289527d6ea49ccb8f7","e2d3f1a7cb7f48fda3228f01311b3cf0","b74d678842924c41b8bf31951b8edc4b","359262150cad47c28dc6d3a62f212e22","e3dbb0f7d487428fa20af29b6fa87109","5015555688b34934ae2ed649ffd1723a","f6d5c4b982b14ba395704b56e5dc94c5","b3e9c01b7add438cae0605016c182118","69ddd4009bfa451ea53699d2e7d6bec2","98937f4cd555462698431ad155735f08","407003895b2f43d7852717df5870296f"]},"id":"b64fb204","outputId":"63f71a80-a676-4a34-f423-8401b074569e","executionInfo":{"status":"ok","timestamp":1692470071674,"user_tz":240,"elapsed":89337,"user":{"displayName":"Thomas Sebastian","userId":"15795710734502513346"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=0dcc1e274c1dad8e9df244f59665300a89dbeeeb489ee4e980ea07d44479832a\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Mounted at /content/drive\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7f60b54f6494ad28aeeaf92fc253968"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56980cf7f0334d8ca730d45b8a2517d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bae2276258d49caa2a8b496c503abf7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea5618845fe49289527d6ea49ccb8f7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","   Fake News       0.53      0.84      0.65        19\n","   Real News       0.67      0.30      0.41        20\n","\n","    accuracy                           0.56        39\n","   macro avg       0.60      0.57      0.53        39\n","weighted avg       0.60      0.56      0.53        39\n","\n"]}],"source":["import importlib\n","\n","package = \"tensorflow\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install - U {package}\n","    importlib.import_module(package)\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"pandas\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"sklearn\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"keras\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"torch\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"langdetect\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","\n","import re\n","from langdetect import detect\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/news_200.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/generated_combined_data200text-davinci-002.csv\")\n","\n","\n","df = df.dropna(subset=['text'])\n","df = df.dropna(subset=['title'])\n","\n","df1 = df1.dropna(subset=['text'])\n","df1 = df1.dropna(subset=['title'])\n","\n","# Step 3: BERT Tokenization & Formatting\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","def tokenize_text(dfx, max_len):\n","    return tokenizer.batch_encode_plus(\n","        dfx['text'].tolist(),\n","        max_length = max_len,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        return_token_type_ids=False\n","    )\n","\n","# Tokenize the text\n","max_len = 256  # choose a max length\n","tokens = tokenize_text(df, max_len)\n","tokens1 = tokenize_text(df1, max_len)\n","\n","# Step 4: Model Creation\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=2,\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","\n","model = model.to(device)\n","\n","# Split into training and testing datasets\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(tokens['input_ids'], df['label'].values, random_state=100, test_size=0.2)\n","\n","# Split into training and testing datasets\n","train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(tokens1['input_ids'], df1['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs, train_labels = train_inputs1, train_labels1\n","\n","\n","# Convert into torch tensors\n","train_inputs = torch.tensor(train_inputs)\n","test_inputs = torch.tensor(test_inputs)\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","# Create DataLoader for the training set\n","train_data = TensorDataset(train_inputs, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=32)\n","\n","# Step 5: Training\n","#optimizer = AdamW(model.parameters(), lr=1e-5)\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5)  # Add weight decay parameter\n","\n","\n","for epoch in range(1):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)  # Add weight decay parameter\n","\n","for epoch in range(3):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","\n","        # Apply weight decay\n","        #l2_regularization = torch.tensor(0.)\n","        l2_regularization = torch.tensor(0., device=device)\n","        for param in model.parameters():\n","            l2_regularization += torch.norm(param, p=2)\n","\n","\n","        loss += 0.01 * l2_regularization  # Adjust the weight decay factor\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# Create DataLoader for the test set\n","test_data = TensorDataset(test_inputs, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=32)\n","\n","# Step 6: Evaluation\n","model.eval()\n","\n","predictions = []\n","true_labels = []\n","\n","for batch in test_dataloader:\n","    batch = [b.to(device) for b in batch]\n","    b_input_ids, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","\n","    # Move logits and labels to CPU\n","    logits = outputs[0].detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# For each input batch, pick the label (0 or 1) with the higher score\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","\n","# Print the classification report\n","print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"],"id":"b64fb204"},{"cell_type":"code","source":["import importlib\n","\n","package = \"tensorflow\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install - U {package}\n","    importlib.import_module(package)\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"pandas\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"sklearn\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"keras\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"torch\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"langdetect\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","\n","import re\n","from langdetect import detect\n","\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/news_500.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/generated_combined_data500text-davinci-002.csv\")\n","\n","\n","df = df.dropna(subset=['text'])\n","df = df.dropna(subset=['title'])\n","\n","df1 = df1.dropna(subset=['text'])\n","df1 = df1.dropna(subset=['title'])\n","\n","# Step 3: BERT Tokenization & Formatting\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","def tokenize_text(dfx, max_len):\n","    return tokenizer.batch_encode_plus(\n","        dfx['text'].tolist(),\n","        max_length = max_len,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        return_token_type_ids=False\n","    )\n","\n","# Tokenize the text\n","max_len = 256  # choose a max length\n","tokens = tokenize_text(df, max_len)\n","tokens1 = tokenize_text(df1, max_len)\n","\n","# Step 4: Model Creation\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=2,\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","\n","model = model.to(device)\n","\n","# Split into training and testing datasets\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(tokens['input_ids'], df['label'].values, random_state=100, test_size=0.2)\n","\n","# Split into training and testing datasets\n","train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(tokens1['input_ids'], df1['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs, train_labels = train_inputs1, train_labels1\n","\n","\n","# Convert into torch tensors\n","train_inputs = torch.tensor(train_inputs)\n","test_inputs = torch.tensor(test_inputs)\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","# Create DataLoader for the training set\n","train_data = TensorDataset(train_inputs, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=32)\n","\n","# Step 5: Training\n","#optimizer = AdamW(model.parameters(), lr=1e-5)\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5)  # Add weight decay parameter\n","\n","\n","for epoch in range(1):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)  # Add weight decay parameter\n","\n","for epoch in range(3):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","\n","        # Apply weight decay\n","        #l2_regularization = torch.tensor(0.)\n","        l2_regularization = torch.tensor(0., device=device)\n","        for param in model.parameters():\n","            l2_regularization += torch.norm(param, p=2)\n","\n","\n","        loss += 0.01 * l2_regularization  # Adjust the weight decay factor\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# Create DataLoader for the test set\n","test_data = TensorDataset(test_inputs, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=32)\n","\n","# Step 6: Evaluation\n","model.eval()\n","\n","predictions = []\n","true_labels = []\n","\n","for batch in test_dataloader:\n","    batch = [b.to(device) for b in batch]\n","    b_input_ids, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","\n","    # Move logits and labels to CPU\n","    logits = outputs[0].detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# For each input batch, pick the label (0 or 1) with the higher score\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","\n","# Print the classification report\n","print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ovZFg29tOUYy","executionInfo":{"status":"ok","timestamp":1692470137300,"user_tz":240,"elapsed":65639,"user":{"displayName":"Thomas Sebastian","userId":"15795710734502513346"}},"outputId":"a8b544bb-5f51-4856-c4b7-99193d969b6d"},"id":"ovZFg29tOUYy","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","   Fake News       0.57      0.93      0.70        54\n","   Real News       0.64      0.16      0.25        45\n","\n","    accuracy                           0.58        99\n","   macro avg       0.60      0.54      0.48        99\n","weighted avg       0.60      0.58      0.50        99\n","\n"]}]},{"cell_type":"code","source":["import importlib\n","\n","package = \"tensorflow\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install - U {package}\n","    importlib.import_module(package)\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"pandas\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"sklearn\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"keras\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"torch\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"langdetect\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","\n","import re\n","from langdetect import detect\n","\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/news_10000.csv')\n","df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/train_davinci_10000.csv')\n","\n","\n","df = df.dropna(subset=['text'])\n","df = df.dropna(subset=['title'])\n","\n","df1 = df1.dropna(subset=['text'])\n","df1 = df1.dropna(subset=['title'])\n","\n","# Step 3: BERT Tokenization & Formatting\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","def tokenize_text(dfx, max_len):\n","    return tokenizer.batch_encode_plus(\n","        dfx['text'].tolist(),\n","        max_length = max_len,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        return_token_type_ids=False\n","    )\n","\n","# Tokenize the text\n","max_len = 256  # choose a max length\n","tokens = tokenize_text(df, max_len)\n","tokens1 = tokenize_text(df1, max_len)\n","\n","# Step 4: Model Creation\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=2,\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","\n","model = model.to(device)\n","\n","# Split into training and testing datasets\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(tokens['input_ids'], df['label'].values, random_state=100, test_size=0.2)\n","\n","# Split into training and testing datasets\n","train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(tokens1['input_ids'], df1['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs, train_labels = train_inputs1, train_labels1\n","\n","\n","# Convert into torch tensors\n","train_inputs = torch.tensor(train_inputs)\n","test_inputs = torch.tensor(test_inputs)\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","# Create DataLoader for the training set\n","train_data = TensorDataset(train_inputs, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=32)\n","\n","# Step 5: Training\n","#optimizer = AdamW(model.parameters(), lr=1e-5)\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5)  # Add weight decay parameter\n","\n","\n","for epoch in range(1):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)  # Add weight decay parameter\n","\n","for epoch in range(3):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","\n","        # Apply weight decay\n","        #l2_regularization = torch.tensor(0.)\n","        l2_regularization = torch.tensor(0., device=device)\n","        for param in model.parameters():\n","            l2_regularization += torch.norm(param, p=2)\n","\n","\n","        loss += 0.01 * l2_regularization  # Adjust the weight decay factor\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# Create DataLoader for the test set\n","test_data = TensorDataset(test_inputs, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=32)\n","\n","# Step 6: Evaluation\n","model.eval()\n","\n","predictions = []\n","true_labels = []\n","\n","for batch in test_dataloader:\n","    batch = [b.to(device) for b in batch]\n","    b_input_ids, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","\n","    # Move logits and labels to CPU\n","    logits = outputs[0].detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# For each input batch, pick the label (0 or 1) with the higher score\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","\n","# Print the classification report\n","print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EvKan2NpOmPH","executionInfo":{"status":"ok","timestamp":1692471461900,"user_tz":240,"elapsed":1323377,"user":{"displayName":"Thomas Sebastian","userId":"15795710734502513346"}},"outputId":"b72557da-15a3-4404-e167-afd225e06577"},"id":"EvKan2NpOmPH","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","   Fake News       0.55      0.98      0.70      1029\n","   Real News       0.82      0.08      0.14       912\n","\n","    accuracy                           0.56      1941\n","   macro avg       0.68      0.53      0.42      1941\n","weighted avg       0.67      0.56      0.44      1941\n","\n"]}]},{"cell_type":"code","source":["import importlib\n","\n","package = \"tensorflow\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install - U {package}\n","    importlib.import_module(package)\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"pandas\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"sklearn\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"keras\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"torch\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"langdetect\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","\n","import re\n","from langdetect import detect\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/train.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Davinci/train_davinci_20400.csv\")\n","\n","\n","\n","df = df.dropna(subset=['text'])\n","df = df.dropna(subset=['title'])\n","\n","df1 = df1.dropna(subset=['text'])\n","df1 = df1.dropna(subset=['title'])\n","\n","# Step 3: BERT Tokenization & Formatting\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","def tokenize_text(dfx, max_len):\n","    return tokenizer.batch_encode_plus(\n","        dfx['text'].tolist(),\n","        max_length = max_len,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        return_token_type_ids=False\n","    )\n","\n","# Tokenize the text\n","max_len = 256  # choose a max length\n","tokens = tokenize_text(df, max_len)\n","tokens1 = tokenize_text(df1, max_len)\n","\n","# Step 4: Model Creation\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=2,\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","\n","model = model.to(device)\n","\n","# Split into training and testing datasets\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(tokens['input_ids'], df['label'].values, random_state=100, test_size=0.2)\n","\n","# Split into training and testing datasets\n","train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(tokens1['input_ids'], df1['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs, train_labels = train_inputs1, train_labels1\n","\n","\n","# Convert into torch tensors\n","train_inputs = torch.tensor(train_inputs)\n","test_inputs = torch.tensor(test_inputs)\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","# Create DataLoader for the training set\n","train_data = TensorDataset(train_inputs, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=32)\n","\n","# Step 5: Training\n","#optimizer = AdamW(model.parameters(), lr=1e-5)\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5)  # Add weight decay parameter\n","\n","\n","for epoch in range(1):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)  # Add weight decay parameter\n","\n","for epoch in range(3):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","\n","        # Apply weight decay\n","        #l2_regularization = torch.tensor(0.)\n","        l2_regularization = torch.tensor(0., device=device)\n","        for param in model.parameters():\n","            l2_regularization += torch.norm(param, p=2)\n","\n","\n","        loss += 0.01 * l2_regularization  # Adjust the weight decay factor\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# Create DataLoader for the test set\n","test_data = TensorDataset(test_inputs, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=32)\n","\n","# Step 6: Evaluation\n","model.eval()\n","\n","predictions = []\n","true_labels = []\n","\n","for batch in test_dataloader:\n","    batch = [b.to(device) for b in batch]\n","    b_input_ids, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","\n","    # Move logits and labels to CPU\n","    logits = outputs[0].detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# For each input batch, pick the label (0 or 1) with the higher score\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","\n","# Print the classification report\n","print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1105d61607db4cf58fb30ba2964e5520","ec784329bdbd421fbf4887dfe271240a","bacdec85572e4e0e823e3e938b85b156","df1475568ab84034a04da1ade6c22101","edb6aa2c60cd49f1a284acb85b8bdb2b","845b43ab112d49749820980c24fe3d65","43b13b8569d84c92b4564a322fba0483","93009225e38b48c0b6465adaaaef0608","b9891d77d9434718bcb386fe3c05ae6b","681b3c0e941e440b8ae65f0bb11ace41","6690045db96c4362b86631bf2edc2f35","eb80f036f7734f10ada7e29ab2b3bcac","c1c534b8af85459c96b02629a34b4842","6ecd33300aff415ba42d68edcd200c01","162f7cff68124e94ba3ba6fba94cfe53","6700bf9db0894ecaa9037ff7f8dc07ab","349b022ccb504b19a179db1cad3384b1","d1d0d4b5275f4eb3bb037d5cf23fa32a","01cf622e07674f9fa14ef268351ede01","2d73296886d14a2f869c37075ae5a84f","479bc59820db48ffa14780483268632d","f43f83465f7d4444a114d4e57fe74686","b42e40f2ec4c45f98c89080a9675aa77","accf865791c943138606ba178136e7c3","508e4de77da041feb1c4a831a3f7d8d3","f8b5b63a47404af099cfd4ac580a6692","4376fdaf3036440c9a660ec9b576e7a1","ad616427bc534d68b1b3b968e7edf62e","6fc862fe74a1427e9478102d6b4a4eb7","0ab6b3d61263422e97d6969845f653e1","94109af64db24e419e8c2e3b7ede66d6","31285678604a454191f10ce972ba0880","ea5ec4f332d243ce9093de922bf675c7","d2f7ce508830449dacb29307460aa98a","aa44abb897554ad8aa705939aed78163","68bcb0e0771e45128bb6179a0a0c4df8","ab07781131da4d1697c1471a893f7cd7","b879ac985f7e44aea792600e1b6db5ff","a9da25eadd094e16951d5db2eb95e7e7","9b118b467e3f4911b8f4706cd5b8aa33","e9d1c9dadb1d4e62af7668a54ecf5ff8","effe3237b26144f1ab4197bc75c778d7","51aaf5ebd21f4121bc397a8778c2bbb2","c0a6c7518cc74ad19a8bf1384ca94346"]},"id":"YYJf1Z37OuKw","executionInfo":{"status":"ok","timestamp":1692473767969,"user_tz":240,"elapsed":1127951,"user":{"displayName":"Thomas Sebastian","userId":"15795710734502513346"}},"outputId":"926ce169-82cb-43d1-97cb-e853c7242163"},"id":"YYJf1Z37OuKw","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=5dead663d36b97a16c4e521026fa6802ab71935ed56adaecc4a3d6e12e87ee66\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Mounted at /content/drive\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1105d61607db4cf58fb30ba2964e5520"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb80f036f7734f10ada7e29ab2b3bcac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b42e40f2ec4c45f98c89080a9675aa77"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2f7ce508830449dacb29307460aa98a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","   Fake News       0.55      0.90      0.68      2104\n","   Real News       0.64      0.19      0.29      1937\n","\n","    accuracy                           0.56      4041\n","   macro avg       0.59      0.55      0.49      4041\n","weighted avg       0.59      0.56      0.49      4041\n","\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1lYob1rPfGSf7FeXENk7RLqGyEy8uoHWE","timestamp":1690770955508},{"file_id":"1r20xJXSPGmjU6w71C2RIhMcnrp-VBYDR","timestamp":1690744032615},{"file_id":"1bdEsmb0UwC4fZ1zrRmjJb1RZKp1EYSy2","timestamp":1688930951314},{"file_id":"17C63qnZO6bvkANqwOyyPzEUNe0ZJluuL","timestamp":1688929870555},{"file_id":"1-B2oiTgXLFYEan4mLoHuEE2kBMXFkHv9","timestamp":1688928661031},{"file_id":"12Nnjz2ldFUJBms_l-pSmAwXkbX4G0mZE","timestamp":1688926225691},{"file_id":"1YTOxD9jEmm91uAbuAyg50qH5TNEUPXhX","timestamp":1688904820636},{"file_id":"1xovG9wwGJp1U2_a3wEk7ts4ie_x9fNqA","timestamp":1686508547571}],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d7f60b54f6494ad28aeeaf92fc253968":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_346e69ab6ba14ec18e71cc415a8fd982","IPY_MODEL_1e9084bdfee84b36bc3e051dcd29b6aa","IPY_MODEL_0158b2d45fce45d5af5c6eaae13790ab"],"layout":"IPY_MODEL_a36bb37d18b24099a14303b8eed565e7"}},"346e69ab6ba14ec18e71cc415a8fd982":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61feb8abe662418e9bf5d65be6f95a0c","placeholder":"​","style":"IPY_MODEL_3dd383bddfad4d1e840609b15b859d99","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"1e9084bdfee84b36bc3e051dcd29b6aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a2df728a8b54f13a437642726e6d5a7","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0969066a3a6441fb150b606b473af37","value":231508}},"0158b2d45fce45d5af5c6eaae13790ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_438483f07eae4b18817335557db4d078","placeholder":"​","style":"IPY_MODEL_238c1c773499433796df4954e4b16af4","value":" 232k/232k [00:00&lt;00:00, 13.5MB/s]"}},"a36bb37d18b24099a14303b8eed565e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61feb8abe662418e9bf5d65be6f95a0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dd383bddfad4d1e840609b15b859d99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a2df728a8b54f13a437642726e6d5a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0969066a3a6441fb150b606b473af37":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"438483f07eae4b18817335557db4d078":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"238c1c773499433796df4954e4b16af4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56980cf7f0334d8ca730d45b8a2517d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14de9d6de8f24eddac2133e22a7e4411","IPY_MODEL_e086f878c508494d933dcfbb8d345b27","IPY_MODEL_9c8da210af0048e786e352c9e4d9638b"],"layout":"IPY_MODEL_eb6fd7f5bfb3466ab2fb59db7f7ac317"}},"14de9d6de8f24eddac2133e22a7e4411":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ded24a81058944dfa36e5b3bc10896cd","placeholder":"​","style":"IPY_MODEL_828d3c399ba14dbc9b37606c193dc4b5","value":"Downloading (…)okenizer_config.json: 100%"}},"e086f878c508494d933dcfbb8d345b27":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9714662d14e4142ac05a9a34f289f9d","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6de826524e65489a8c3435a661c43d40","value":28}},"9c8da210af0048e786e352c9e4d9638b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eac8aa8ed6ce454eae24b708ee8b4e7e","placeholder":"​","style":"IPY_MODEL_9a0eafee9266489996ea31c6e18ae8b7","value":" 28.0/28.0 [00:00&lt;00:00, 2.37kB/s]"}},"eb6fd7f5bfb3466ab2fb59db7f7ac317":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ded24a81058944dfa36e5b3bc10896cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"828d3c399ba14dbc9b37606c193dc4b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9714662d14e4142ac05a9a34f289f9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6de826524e65489a8c3435a661c43d40":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eac8aa8ed6ce454eae24b708ee8b4e7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a0eafee9266489996ea31c6e18ae8b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1bae2276258d49caa2a8b496c503abf7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6147f310184f4dc2b4aedfcc1758ad76","IPY_MODEL_f940317571df42519017da214bc2b813","IPY_MODEL_e1308300d83c4666ac36b16a1266d888"],"layout":"IPY_MODEL_82412c38c46a47839cdbec61835ecc59"}},"6147f310184f4dc2b4aedfcc1758ad76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f360bc4994e4d2ea8d555c5618797be","placeholder":"​","style":"IPY_MODEL_83f994d19f504c568f9d96e6ab5f5ee0","value":"Downloading (…)lve/main/config.json: 100%"}},"f940317571df42519017da214bc2b813":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dabd4c6fa9de422187f6732d1337bba1","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f46abf0835a40ea9b621a038ba3d00e","value":570}},"e1308300d83c4666ac36b16a1266d888":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdab23fb702c4c36bad607fd78e492c7","placeholder":"​","style":"IPY_MODEL_5357cf03e2fc45268dde6cda296e61f2","value":" 570/570 [00:00&lt;00:00, 47.1kB/s]"}},"82412c38c46a47839cdbec61835ecc59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f360bc4994e4d2ea8d555c5618797be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83f994d19f504c568f9d96e6ab5f5ee0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dabd4c6fa9de422187f6732d1337bba1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f46abf0835a40ea9b621a038ba3d00e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cdab23fb702c4c36bad607fd78e492c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5357cf03e2fc45268dde6cda296e61f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ea5618845fe49289527d6ea49ccb8f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2d3f1a7cb7f48fda3228f01311b3cf0","IPY_MODEL_b74d678842924c41b8bf31951b8edc4b","IPY_MODEL_359262150cad47c28dc6d3a62f212e22"],"layout":"IPY_MODEL_e3dbb0f7d487428fa20af29b6fa87109"}},"e2d3f1a7cb7f48fda3228f01311b3cf0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5015555688b34934ae2ed649ffd1723a","placeholder":"​","style":"IPY_MODEL_f6d5c4b982b14ba395704b56e5dc94c5","value":"Downloading model.safetensors: 100%"}},"b74d678842924c41b8bf31951b8edc4b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3e9c01b7add438cae0605016c182118","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69ddd4009bfa451ea53699d2e7d6bec2","value":440449768}},"359262150cad47c28dc6d3a62f212e22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98937f4cd555462698431ad155735f08","placeholder":"​","style":"IPY_MODEL_407003895b2f43d7852717df5870296f","value":" 440M/440M [00:00&lt;00:00, 533MB/s]"}},"e3dbb0f7d487428fa20af29b6fa87109":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5015555688b34934ae2ed649ffd1723a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6d5c4b982b14ba395704b56e5dc94c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3e9c01b7add438cae0605016c182118":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69ddd4009bfa451ea53699d2e7d6bec2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98937f4cd555462698431ad155735f08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"407003895b2f43d7852717df5870296f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1105d61607db4cf58fb30ba2964e5520":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec784329bdbd421fbf4887dfe271240a","IPY_MODEL_bacdec85572e4e0e823e3e938b85b156","IPY_MODEL_df1475568ab84034a04da1ade6c22101"],"layout":"IPY_MODEL_edb6aa2c60cd49f1a284acb85b8bdb2b"}},"ec784329bdbd421fbf4887dfe271240a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_845b43ab112d49749820980c24fe3d65","placeholder":"​","style":"IPY_MODEL_43b13b8569d84c92b4564a322fba0483","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"bacdec85572e4e0e823e3e938b85b156":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93009225e38b48c0b6465adaaaef0608","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b9891d77d9434718bcb386fe3c05ae6b","value":231508}},"df1475568ab84034a04da1ade6c22101":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_681b3c0e941e440b8ae65f0bb11ace41","placeholder":"​","style":"IPY_MODEL_6690045db96c4362b86631bf2edc2f35","value":" 232k/232k [00:00&lt;00:00, 12.0MB/s]"}},"edb6aa2c60cd49f1a284acb85b8bdb2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"845b43ab112d49749820980c24fe3d65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43b13b8569d84c92b4564a322fba0483":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93009225e38b48c0b6465adaaaef0608":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9891d77d9434718bcb386fe3c05ae6b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"681b3c0e941e440b8ae65f0bb11ace41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6690045db96c4362b86631bf2edc2f35":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb80f036f7734f10ada7e29ab2b3bcac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1c534b8af85459c96b02629a34b4842","IPY_MODEL_6ecd33300aff415ba42d68edcd200c01","IPY_MODEL_162f7cff68124e94ba3ba6fba94cfe53"],"layout":"IPY_MODEL_6700bf9db0894ecaa9037ff7f8dc07ab"}},"c1c534b8af85459c96b02629a34b4842":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_349b022ccb504b19a179db1cad3384b1","placeholder":"​","style":"IPY_MODEL_d1d0d4b5275f4eb3bb037d5cf23fa32a","value":"Downloading (…)okenizer_config.json: 100%"}},"6ecd33300aff415ba42d68edcd200c01":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_01cf622e07674f9fa14ef268351ede01","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d73296886d14a2f869c37075ae5a84f","value":28}},"162f7cff68124e94ba3ba6fba94cfe53":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_479bc59820db48ffa14780483268632d","placeholder":"​","style":"IPY_MODEL_f43f83465f7d4444a114d4e57fe74686","value":" 28.0/28.0 [00:00&lt;00:00, 2.72kB/s]"}},"6700bf9db0894ecaa9037ff7f8dc07ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"349b022ccb504b19a179db1cad3384b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1d0d4b5275f4eb3bb037d5cf23fa32a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01cf622e07674f9fa14ef268351ede01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d73296886d14a2f869c37075ae5a84f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"479bc59820db48ffa14780483268632d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f43f83465f7d4444a114d4e57fe74686":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b42e40f2ec4c45f98c89080a9675aa77":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_accf865791c943138606ba178136e7c3","IPY_MODEL_508e4de77da041feb1c4a831a3f7d8d3","IPY_MODEL_f8b5b63a47404af099cfd4ac580a6692"],"layout":"IPY_MODEL_4376fdaf3036440c9a660ec9b576e7a1"}},"accf865791c943138606ba178136e7c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad616427bc534d68b1b3b968e7edf62e","placeholder":"​","style":"IPY_MODEL_6fc862fe74a1427e9478102d6b4a4eb7","value":"Downloading (…)lve/main/config.json: 100%"}},"508e4de77da041feb1c4a831a3f7d8d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ab6b3d61263422e97d6969845f653e1","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_94109af64db24e419e8c2e3b7ede66d6","value":570}},"f8b5b63a47404af099cfd4ac580a6692":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31285678604a454191f10ce972ba0880","placeholder":"​","style":"IPY_MODEL_ea5ec4f332d243ce9093de922bf675c7","value":" 570/570 [00:00&lt;00:00, 48.3kB/s]"}},"4376fdaf3036440c9a660ec9b576e7a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad616427bc534d68b1b3b968e7edf62e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fc862fe74a1427e9478102d6b4a4eb7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ab6b3d61263422e97d6969845f653e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94109af64db24e419e8c2e3b7ede66d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"31285678604a454191f10ce972ba0880":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea5ec4f332d243ce9093de922bf675c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2f7ce508830449dacb29307460aa98a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa44abb897554ad8aa705939aed78163","IPY_MODEL_68bcb0e0771e45128bb6179a0a0c4df8","IPY_MODEL_ab07781131da4d1697c1471a893f7cd7"],"layout":"IPY_MODEL_b879ac985f7e44aea792600e1b6db5ff"}},"aa44abb897554ad8aa705939aed78163":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9da25eadd094e16951d5db2eb95e7e7","placeholder":"​","style":"IPY_MODEL_9b118b467e3f4911b8f4706cd5b8aa33","value":"Downloading model.safetensors: 100%"}},"68bcb0e0771e45128bb6179a0a0c4df8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9d1c9dadb1d4e62af7668a54ecf5ff8","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_effe3237b26144f1ab4197bc75c778d7","value":440449768}},"ab07781131da4d1697c1471a893f7cd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51aaf5ebd21f4121bc397a8778c2bbb2","placeholder":"​","style":"IPY_MODEL_c0a6c7518cc74ad19a8bf1384ca94346","value":" 440M/440M [00:00&lt;00:00, 500MB/s]"}},"b879ac985f7e44aea792600e1b6db5ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9da25eadd094e16951d5db2eb95e7e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b118b467e3f4911b8f4706cd5b8aa33":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9d1c9dadb1d4e62af7668a54ecf5ff8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"effe3237b26144f1ab4197bc75c778d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"51aaf5ebd21f4121bc397a8778c2bbb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0a6c7518cc74ad19a8bf1384ca94346":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}