{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWLaOrPsgLLS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR API KEY\"\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "\n",
    "package = \"tensorflow\"\n",
    "\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install -U {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"openai\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install --upgrade openai\n",
    "    import openai\n",
    "\n",
    "# Set up OpenAI API\n",
    "API_KEY = \"YOUR API KEY\"\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "# Load the dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/news_200.csv')\n",
    "df = df.dropna(subset=['text', 'title'])\n",
    "\n",
    "# Function to determine the prompt based on the label value\n",
    "def determine_prompt(label_value):\n",
    "    if label_value == 1:\n",
    "        return \"Write a fictional news article that is untrue and never happened in history\"\n",
    "    elif label_value == 0:\n",
    "        return \"Write a news article from the past decade.\"\n",
    "\n",
    "# Convert the dataframe to a list of dictionaries\n",
    "records = []\n",
    "for index, row in df.iterrows():\n",
    "    prompt_text = determine_prompt(row['label'])\n",
    "    ideal_generated_text = row['text']\n",
    "\n",
    "    record = {\n",
    "        \"prompt\": prompt_text,\n",
    "        \"completion\": ideal_generated_text\n",
    "    }\n",
    "\n",
    "    records.append(record)\n",
    "\n",
    "# Convert the records list to a DataFrame\n",
    "df_records = pd.DataFrame(records)\n",
    "if os.path.exists('/content/drive/MyDrive/Colab Notebooks/output_curie.jsonl'):\n",
    "    os.remove('/content/drive/MyDrive/Colab Notebooks/output_curie.jsonl')\n",
    "\n",
    "\n",
    "# Write the DataFrame to a jsonl file\n",
    "df_records.to_json('/content/drive/MyDrive/Colab Notebooks/output_curie.jsonl', orient='records', lines=True)\n",
    "\n",
    "print(\"JSONL file has been created as output.jsonl\")\n",
    "!openai tools fine_tunes.prepare_data -f '/content/drive/MyDrive/Colab Notebooks/output_curie.jsonl'\n",
    "!openai api fine_tunes.create -t '/content/drive/MyDrive/Colab Notebooks/output_curie.jsonl' -m 'curie'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNaG_UCkdQTf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR API KEY\"\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "\n",
    "package = \"tensorflow\"\n",
    "\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install -U {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"openai\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install --upgrade openai\n",
    "    import openai\n",
    "\n",
    "# Set up OpenAI API\n",
    "API_KEY = \"YOUR API KEY\"\n",
    "openai.api_key = API_KEY\n",
    "!openai api fine_tunes.follow -i ft-V72pIZRS3d8MN1Ct9GaGLGIe\n",
    "import csv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0cKnoIE1cFB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR API KEY\"\n",
    "import importlib\n",
    "package = \"openai\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install --upgrade openai\n",
    "    import openai\n",
    "\n",
    "# Set up OpenAI API\n",
    "API_KEY = \"YOUR API KEY\"\n",
    "openai.api_key = API_KEY\n",
    "!openai api completions.create -m \"your fine tuned key\"7 -p \"Write a fictional news article that is untrue and never happened in history->\" -M 256 -t 0.7\n",
    "\n",
    "\n",
    "import csv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1zqDNRS6RFz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR API KEY\"\n",
    "import importlib\n",
    "package = \"openai\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install --upgrade openai\n",
    "    import openai\n",
    "\n",
    "# Set up OpenAI API\n",
    "API_KEY = \"YOUR API KEY\"\n",
    "openai.api_key = API_KEY\n",
    "!openai api completions.create -m \"YOUR FINE TUNED KEY\" -p \"Write a news article from the past decade.->\" -M 256 -t 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52-NwiZPzLlv"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "package = \"tensorflow\"\n",
    "\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install - U {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"openai\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install --upgrade openai\n",
    "    import openai\n",
    "\n",
    "# Set up OpenAI API\n",
    "API_KEY = \"YOUR API KEY\"\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "import csv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# Specify the file paths\n",
    "generated_fake_data_file = \"/content/drive/MyDrive/Colab Notebooks/generated_fake_data100text-\"your fine tuned model\".csv\"\n",
    "generated_real_data_file = \"/content/drive/MyDrive/Colab Notebooks/generated_real_data100text-\"your fine tuned model\".csv\"\n",
    "combined_data_file = \"/content/drive/MyDrive/Colab Notebooks/generated_combined_data200text-\"your fine tuned model\".csv\"\n",
    "\n",
    "# Delete generated_fake_datatext-curie-001.csv if it exists\n",
    "if os.path.exists(generated_fake_data_file):\n",
    "    os.remove(generated_fake_data_file)\n",
    "\n",
    "# Delete generated_real_datatext-curie-001.csv if it exists\n",
    "if os.path.exists(generated_real_data_file):\n",
    "    os.remove(generated_real_data_file)\n",
    "\n",
    "# Delete combined_datatext-curie-001.csv if it exists\n",
    "if os.path.exists(combined_data_file):\n",
    "    os.remove(combined_data_file)\n",
    "\n",
    "\n",
    "# This function generates a text based on the given prompt\n",
    "def generate_text(prompt, temperature=0.7, max_tokens=200):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"your fine tuned model\",\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# A list to store the generated news\n",
    "news_data = []\n",
    "fake_news_data = []\n",
    "\n",
    "\n",
    "# Generate 10400 lines of news (real and fake)\n",
    "for i in range(200):\n",
    "    # Real News\n",
    "    real_title = generate_text(\"Write a title for a news article from the past decade.->\")\n",
    "    real_text = generate_text(\"Write a news article from the past decade.->\")\n",
    "    news_data.append({'id': i, 'title': real_title, 'author': 'AI', 'text': real_text, 'label': '0'})\n",
    "\n",
    "    # Fake News\n",
    "    fake_title = generate_text(\"Write a title for fictional news article that has never happened in history->\")\n",
    "    fake_text = generate_text(\"Write a fictional news article that is untrue and never happened in history->\")\n",
    "    fake_news_data.append({'id': i, 'title': fake_title, 'author': 'AI', 'text': fake_text, 'label': '1'})\n",
    "\n",
    "    # Save the real news data to CSV file for each iteration\n",
    "    with open(generated_real_data_file, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'author', 'text', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if i == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(news_data[-1])\n",
    "\n",
    "    # Save the fake news data to CSV file for each iteration\n",
    "    with open(generated_fake_data_file, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'author', 'text', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if i == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(fake_news_data[-1])\n",
    "\n",
    "    # Combine the two datasets for each iteration\n",
    "    combined_data = fake_news_data + news_data\n",
    "\n",
    "    # Save the combined_data as a CSV file for each iteration\n",
    "    with open(combined_data_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'author', 'text', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMOQJmFE1q5S"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "package = \"tensorflow\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install - U {package}\n",
    "    importlib.import_module(package)\n",
    "package = \"transformers\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"pandas\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"sklearn\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"keras\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"torch\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"transformers\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"langdetect\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check for the availability of required packages and install it\n",
    "required_packages = ['tensorflow', 'transformers', 'pandas', 'sklearn', 'keras', 'torch', 'langdetect']\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        !pip install {package}\n",
    "        importlib.import_module(package)\n",
    "\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Fine_Tuned_Curie _Few_Shot_Data_Generation_and_Roberta/news_200.csv')\n",
    "df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Fine_Tuned_Curie _Few_Shot_Data_Generation_and_Roberta/generated_combined_data200text-curie:ft-university-of-galway-2023-08-10-07-43-47.csv\")\n",
    "\n",
    "\n",
    "df = df.dropna(subset=['text'])\n",
    "df = df.dropna(subset=['title'])\n",
    "\n",
    "df1 = df1.dropna(subset=['text'])\n",
    "df1 = df1.dropna(subset=['title'])\n",
    "\n",
    "# Tokenization and Formatting\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "\n",
    "def tokenize_text(dfx, max_len):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        dfx['text'].str.lower().tolist(),\n",
    "        max_length = max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "max_len = 256  #max length\n",
    "tokens = tokenize_text(df, max_len)\n",
    "tokens1 = tokenize_text(df1, max_len)\n",
    "\n",
    "# Model Creation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(tokens['input_ids'], df['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(tokens1['input_ids'], df1['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "train_inputs, train_labels = train_inputs1, train_labels1\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(train_inputs, train_labels, random_state=100, test_size=0.1)\n",
    "\n",
    "# Convert into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create DataLoader for the training, testing and validation set\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=32,shuffle  = True)\n",
    "val_data = TensorDataset(val_inputs, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)  # Add weight decay parameter\n",
    "\n",
    "best_acc = 0\n",
    "tolerance = 0\n",
    "# Keep track of the validation accuracy for each epoch\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(20):  # maximum of 100 epochs\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Apply weight decay\n",
    "        l2_regularization = torch.tensor(0., device=device)\n",
    "        for param in model.parameters():\n",
    "            l2_regularization += torch.norm(param, p=2)\n",
    "\n",
    "        loss += 0.01 * l2_regularization  # Adjust the weight decay factor\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # After each epoch, evaluate on validation set\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for batch in val_dataloader:\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        b_input_ids, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids)\n",
    "        logits = outputs[0].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "    # Check accuracy\n",
    "    acc = accuracy_score(true_labels, pred_flat)\n",
    "    val_acc_list.append(acc)\n",
    "\n",
    "    print(f'Validation accuracy: {acc}, epoch: {epoch}')\n",
    "\n",
    "    # If this epoch is better than before, reset patience\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        tolerance = 0\n",
    "    else:\n",
    "        # If this epoch did not improve accuracy, increase patience\n",
    "        tolerance += 1\n",
    "        if tolerance > 3:  # no improvement for 3 consecutive epochs\n",
    "            print('Early stopping due to no improvement')\n",
    "            break\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.plot(val_acc_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy vs. Number of Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = [b.to(device) for b in batch]\n",
    "    b_input_ids, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = outputs[0].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# For each input batch, pick the label (0 or 1) with the higher score\n",
    "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSxOqfSsD4FX"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "package = \"tensorflow\"\n",
    "\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install - U {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"openai\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install --upgrade openai\n",
    "    import openai\n",
    "\n",
    "# Set up OpenAI API\n",
    "API_KEY = \"YOUR API KEY\"\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "import csv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# Specify the file paths\n",
    "generated_fake_data_file = \"/content/drive/MyDrive/Colab Notebooks/generated_fake_data250text-curie:ft-university-of-galway-2023-08-10-07-43-47.csv\"\n",
    "generated_real_data_file = \"/content/drive/MyDrive/Colab Notebooks/generated_real_data250text-curie:ft-university-of-galway-2023-08-10-07-43-47.csv\"\n",
    "combined_data_file = \"/content/drive/MyDrive/Colab Notebooks/generated_combined_data500text-curie:ft-university-of-galway-2023-08-10-07-43-47.csv\"\n",
    "\n",
    "# Delete generated_fake_datatext-curie-001.csv if it exists\n",
    "if os.path.exists(generated_fake_data_file):\n",
    "    os.remove(generated_fake_data_file)\n",
    "\n",
    "# Delete generated_real_datatext-curie-001.csv if it exists\n",
    "if os.path.exists(generated_real_data_file):\n",
    "    os.remove(generated_real_data_file)\n",
    "\n",
    "# Delete combined_datatext-curie-001.csv if it exists\n",
    "if os.path.exists(combined_data_file):\n",
    "    os.remove(combined_data_file)\n",
    "\n",
    "\n",
    "# This function generates a text based on the given prompt\n",
    "def generate_text(prompt, temperature=0.7, max_tokens=200):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"curie:ft-university-of-galway-2023-08-10-07-43-47\",\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# A list to store the generated news\n",
    "news_data = []\n",
    "fake_news_data = []\n",
    "\n",
    "\n",
    "# Generate 10400 lines of news (real and fake)\n",
    "for i in range(200):\n",
    "    # Real News\n",
    "    real_title = generate_text(\"Write a title for a news article from the past decade.->\")\n",
    "    real_text = generate_text(\"Write a news article from the past decade.->\")\n",
    "    news_data.append({'id': i, 'title': real_title, 'author': 'AI', 'text': real_text, 'label': '0'})\n",
    "\n",
    "    # Fake News\n",
    "    fake_title = generate_text(\"Write a title for fictional news article that has never happened in history->\")\n",
    "    fake_text = generate_text(\"Write a fictional news article that is untrue and never happened in history->\")\n",
    "    fake_news_data.append({'id': i, 'title': fake_title, 'author': 'AI', 'text': fake_text, 'label': '1'})\n",
    "\n",
    "    # Save the real news data to CSV file for each iteration\n",
    "    with open(generated_real_data_file, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'author', 'text', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if i == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(news_data[-1])\n",
    "\n",
    "    # Save the fake news data to CSV file for each iteration\n",
    "    with open(generated_fake_data_file, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'author', 'text', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if i == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(fake_news_data[-1])\n",
    "\n",
    "    # Combine the two datasets for each iteration\n",
    "    combined_data = fake_news_data + news_data\n",
    "\n",
    "    # Save the combined_data as a CSV file for each iteration\n",
    "    with open(combined_data_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'author', 'text', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3-s0CikC3t9"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "package = \"tensorflow\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install - U {package}\n",
    "    importlib.import_module(package)\n",
    "package = \"transformers\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"pandas\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"sklearn\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"keras\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"torch\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"transformers\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"langdetect\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check for the availability of required packages and install it\n",
    "required_packages = ['tensorflow', 'transformers', 'pandas', 'sklearn', 'keras', 'torch', 'langdetect']\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        !pip install {package}\n",
    "        importlib.import_module(package)\n",
    "\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Fine_Tuned_Curie _Few_Shot_Data_Generation_and_Roberta/news_500.csv')\n",
    "df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Fine_Tuned_Curie _Few_Shot_Data_Generation_and_Roberta/generated_combined_data500text-curie:ft-university-of-galway-2023-08-10-07-43-47.csv\")\n",
    "\n",
    "\n",
    "df = df.dropna(subset=['text'])\n",
    "df = df.dropna(subset=['title'])\n",
    "\n",
    "df1 = df1.dropna(subset=['text'])\n",
    "df1 = df1.dropna(subset=['title'])\n",
    "\n",
    "# Tokenization and Formatting\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "\n",
    "def tokenize_text(dfx, max_len):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        dfx['text'].str.lower().tolist(),\n",
    "        max_length = max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "max_len = 256  #max length\n",
    "tokens = tokenize_text(df, max_len)\n",
    "tokens1 = tokenize_text(df1, max_len)\n",
    "\n",
    "# Model Creation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(tokens['input_ids'], df['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(tokens1['input_ids'], df1['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "train_inputs, train_labels = train_inputs1, train_labels1\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(train_inputs, train_labels, random_state=100, test_size=0.1)\n",
    "\n",
    "# Convert into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create DataLoader for the training, testing and validation set\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=32,shuffle  = True)\n",
    "val_data = TensorDataset(val_inputs, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)  # Add weight decay parameter\n",
    "\n",
    "best_acc = 0\n",
    "tolerance = 0\n",
    "# Keep track of the validation accuracy for each epoch\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(20):  # maximum of 100 epochs\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Apply weight decay\n",
    "        l2_regularization = torch.tensor(0., device=device)\n",
    "        for param in model.parameters():\n",
    "            l2_regularization += torch.norm(param, p=2)\n",
    "\n",
    "        loss += 0.01 * l2_regularization  # Adjust the weight decay factor\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # After each epoch, evaluate on validation set\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for batch in val_dataloader:\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        b_input_ids, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids)\n",
    "        logits = outputs[0].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "    # Check accuracy\n",
    "    acc = accuracy_score(true_labels, pred_flat)\n",
    "    val_acc_list.append(acc)\n",
    "\n",
    "    print(f'Validation accuracy: {acc}, epoch: {epoch}')\n",
    "\n",
    "    # If this epoch is better than before, reset patience\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        tolerance = 0\n",
    "    else:\n",
    "        # If this epoch did not improve accuracy, increase patience\n",
    "        tolerance += 1\n",
    "        if tolerance > 3:  # no improvement for 3 consecutive epochs\n",
    "            print('Early stopping due to no improvement')\n",
    "            break\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.plot(val_acc_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy vs. Number of Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = [b.to(device) for b in batch]\n",
    "    b_input_ids, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = outputs[0].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# For each input batch, pick the label (0 or 1) with the higher score\n",
    "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RC1BZ98xExEX"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "package = \"tensorflow\"\n",
    "\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install - U {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"openai\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install --upgrade openai\n",
    "    import openai\n",
    "\n",
    "# Set up OpenAI API\n",
    "API_KEY = \"YOUR API KEY\"\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "import csv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# Specify the file paths\n",
    "generated_fake_data_file = \"/content/drive/MyDrive/Colab Notebooks/generated_fake_data5000text-curie:ft-university-of-galway-2023-08-10-07-43-47.csv\"\n",
    "generated_real_data_file = \"/content/drive/MyDrive/Colab Notebooks/generated_real_data5000text-curie:ft-university-of-galway-2023-08-10-07-43-47.csv\"\n",
    "combined_data_file = \"/content/drive/MyDrive/Colab Notebooks/generated_combined_data10000text-curie:ft-university-of-galway-2023-08-10-07-43-47.csv\"\n",
    "\n",
    "# Delete generated_fake_datatext-curie-001.csv if it exists\n",
    "if os.path.exists(generated_fake_data_file):\n",
    "    os.remove(generated_fake_data_file)\n",
    "\n",
    "# Delete generated_real_datatext-curie-001.csv if it exists\n",
    "if os.path.exists(generated_real_data_file):\n",
    "    os.remove(generated_real_data_file)\n",
    "\n",
    "# Delete combined_datatext-curie-001.csv if it exists\n",
    "if os.path.exists(combined_data_file):\n",
    "    os.remove(combined_data_file)\n",
    "\n",
    "\n",
    "# This function generates a text based on the given prompt\n",
    "def generate_text(prompt, temperature=0.7, max_tokens=200):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"curie:ft-university-of-galway-2023-08-10-07-43-47\",\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# A list to store the generated news\n",
    "news_data = []\n",
    "fake_news_data = []\n",
    "\n",
    "\n",
    "# Generate 10400 lines of news (real and fake)\n",
    "for i in range(10000):\n",
    "    # Real News\n",
    "    real_title = generate_text(\"Write a title for a news article from the past decade.->\")\n",
    "    real_text = generate_text(\"Write a news article from the past decade.->\")\n",
    "    news_data.append({'id': i, 'title': real_title, 'author': 'AI', 'text': real_text, 'label': '0'})\n",
    "\n",
    "    # Fake News\n",
    "    fake_title = generate_text(\"Write a title for fictional news article that has never happened in history->\")\n",
    "    fake_text = generate_text(\"Write a fictional news article that is untrue and never happened in history->\")\n",
    "    fake_news_data.append({'id': i, 'title': fake_title, 'author': 'AI', 'text': fake_text, 'label': '1'})\n",
    "\n",
    "    # Save the real news data to CSV file for each iteration\n",
    "    with open(generated_real_data_file, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'author', 'text', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if i == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(news_data[-1])\n",
    "\n",
    "    # Save the fake news data to CSV file for each iteration\n",
    "    with open(generated_fake_data_file, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'author', 'text', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if i == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(fake_news_data[-1])\n",
    "\n",
    "    # Combine the two datasets for each iteration\n",
    "    combined_data = fake_news_data + news_data\n",
    "\n",
    "    # Save the combined_data as a CSV file for each iteration\n",
    "    with open(combined_data_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'author', 'text', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rtx32xQ4E79X"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "package = \"tensorflow\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install - U {package}\n",
    "    importlib.import_module(package)\n",
    "package = \"transformers\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"pandas\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"sklearn\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"keras\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"torch\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"transformers\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"langdetect\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check for the availability of required packages and install it\n",
    "required_packages = ['tensorflow', 'transformers', 'pandas', 'sklearn', 'keras', 'torch', 'langdetect']\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        !pip install {package}\n",
    "        importlib.import_module(package)\n",
    "\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Fine_Tuned_Curie _Few_Shot_Data_Generation_and_Roberta/news_10000.csv')\n",
    "df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Fine_Tuned_Curie _Few_Shot_Data_Generation_and_Roberta/generated_combined_data_train_10000_text-curie_ft-university-of-galway-2023-08-10-07-43-47.csv\")\n",
    "\n",
    "df = df.dropna(subset=['text'])\n",
    "df = df.dropna(subset=['title'])\n",
    "\n",
    "df1 = df1.dropna(subset=['text'])\n",
    "df1 = df1.dropna(subset=['title'])\n",
    "\n",
    "# Tokenization and Formatting\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "\n",
    "def tokenize_text(dfx, max_len):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        dfx['text'].str.lower().tolist(),\n",
    "        max_length = max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "max_len = 256  #max length\n",
    "tokens = tokenize_text(df, max_len)\n",
    "tokens1 = tokenize_text(df1, max_len)\n",
    "\n",
    "# Model Creation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(tokens['input_ids'], df['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(tokens1['input_ids'], df1['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "train_inputs, train_labels = train_inputs1, train_labels1\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(train_inputs, train_labels, random_state=100, test_size=0.1)\n",
    "\n",
    "# Convert into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create DataLoader for the training, testing and validation set\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=32,shuffle  = True)\n",
    "val_data = TensorDataset(val_inputs, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)  # Add weight decay parameter\n",
    "\n",
    "best_acc = 0\n",
    "tolerance = 0\n",
    "# Keep track of the validation accuracy for each epoch\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(20):  # maximum of 100 epochs\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Apply weight decay\n",
    "        l2_regularization = torch.tensor(0., device=device)\n",
    "        for param in model.parameters():\n",
    "            l2_regularization += torch.norm(param, p=2)\n",
    "\n",
    "        loss += 0.01 * l2_regularization  # Adjust the weight decay factor\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # After each epoch, evaluate on validation set\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for batch in val_dataloader:\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        b_input_ids, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids)\n",
    "        logits = outputs[0].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "    # Check accuracy\n",
    "    acc = accuracy_score(true_labels, pred_flat)\n",
    "    val_acc_list.append(acc)\n",
    "\n",
    "    print(f'Validation accuracy: {acc}, epoch: {epoch}')\n",
    "\n",
    "    # If this epoch is better than before, reset patience\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        tolerance = 0\n",
    "    else:\n",
    "        # If this epoch did not improve accuracy, increase patience\n",
    "        tolerance += 1\n",
    "        if tolerance > 3:  # no improvement for 3 consecutive epochs\n",
    "            print('Early stopping due to no improvement')\n",
    "            break\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.plot(val_acc_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy vs. Number of Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = [b.to(device) for b in batch]\n",
    "    b_input_ids, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = outputs[0].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# For each input batch, pick the label (0 or 1) with the higher score\n",
    "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xg8uXvj6UfA0"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "package = \"tensorflow\"\n",
    "\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install - U {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"openai\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install --upgrade openai\n",
    "    import openai\n",
    "\n",
    "# Set up OpenAI API\n",
    "API_KEY = \"YOUR API KEY\"\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "import csv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# Specify the file paths\n",
    "generated_fake_data_file = \"/content/drive/MyDrive/Colab Notebooks/generated_fake_data_train_20400_half_text-curie:ft-university-of-galway-2023-08-10-07-43-47.csv\"\n",
    "generated_real_data_file = \"/content/drive/MyDrive/Colab Notebooks/generated_real_data_train_20400_half_text-curie:ft-university-of-galway-2023-08-10-07-43-47.csv\"\n",
    "combined_data_file = \"/content/drive/MyDrive/Colab Notebooks/generated_combined_data_train_20400_text-curie:ft-university-of-galway-2023-08-10-07-43-47.csv\"\n",
    "\n",
    "# Delete generated_fake_datatext-curie-001.csv if it exists\n",
    "if os.path.exists(generated_fake_data_file):\n",
    "    os.remove(generated_fake_data_file)\n",
    "\n",
    "# Delete generated_real_datatext-curie-001.csv if it exists\n",
    "if os.path.exists(generated_real_data_file):\n",
    "    os.remove(generated_real_data_file)\n",
    "\n",
    "# Delete combined_datatext-curie-001.csv if it exists\n",
    "if os.path.exists(combined_data_file):\n",
    "    os.remove(combined_data_file)\n",
    "\n",
    "\n",
    "# This function generates a text based on the given prompt\n",
    "def generate_text(prompt, temperature=0.7, max_tokens=200):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"curie:ft-university-of-galway-2023-08-10-07-43-47\",\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# A list to store the generated news\n",
    "news_data = []\n",
    "fake_news_data = []\n",
    "\n",
    "\n",
    "# Generate 10400 lines of news (real and fake)\n",
    "for i in range(2000):\n",
    "    # Real News\n",
    "    real_title = generate_text(\"Write a title for a news article from the past decade.->\")\n",
    "    real_text = generate_text(\"Write a news article from the past decade.->\")\n",
    "    news_data.append({'id': i, 'title': real_title, 'author': 'AI', 'text': real_text, 'label': '0'})\n",
    "\n",
    "    # Fake News\n",
    "    fake_title = generate_text(\"Write a title for fictional news article that has never happened in history->\")\n",
    "    fake_text = generate_text(\"Write a fictional news article that is untrue and never happened in history->\")\n",
    "    fake_news_data.append({'id': i, 'title': fake_title, 'author': 'AI', 'text': fake_text, 'label': '1'})\n",
    "\n",
    "    # Save the real news data to CSV file for each iteration\n",
    "    with open(generated_real_data_file, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'author', 'text', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if i == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(news_data[-1])\n",
    "\n",
    "    # Save the fake news data to CSV file for each iteration\n",
    "    with open(generated_fake_data_file, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'author', 'text', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if i == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(fake_news_data[-1])\n",
    "\n",
    "    # Combine the two datasets for each iteration\n",
    "    combined_data = fake_news_data + news_data\n",
    "\n",
    "    # Save the combined_data as a CSV file for each iteration\n",
    "    with open(combined_data_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'title', 'author', 'text', 'label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSXExeUZXH31"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "package = \"tensorflow\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install - U {package}\n",
    "    importlib.import_module(package)\n",
    "package = \"transformers\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"pandas\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"sklearn\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"keras\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"torch\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"transformers\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "package = \"langdetect\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check for the availability of required packages and install it\n",
    "required_packages = ['tensorflow', 'transformers', 'pandas', 'sklearn', 'keras', 'torch', 'langdetect']\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        !pip install {package}\n",
    "        importlib.import_module(package)\n",
    "\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv')\n",
    "df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/generated_combined_data_20400_text-curie_ft-university-of-galway-2023-08-10-07-43-47.csv\")\n",
    "#df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/generated_combined_data_24000_text-curie_ft-university-of-galway-2023-08-10-07-43-47_1_9.csv\")\n",
    "\n",
    "\n",
    "df = df.dropna(subset=['text'])\n",
    "df = df.dropna(subset=['title'])\n",
    "\n",
    "df1 = df1.dropna(subset=['text'])\n",
    "df1 = df1.dropna(subset=['title'])\n",
    "\n",
    "# Tokenization and Formatting\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "\n",
    "def tokenize_text(dfx, max_len):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        dfx['text'].str.lower().tolist(),\n",
    "        max_length = max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "max_len = 256  #max length\n",
    "tokens = tokenize_text(df, max_len)\n",
    "tokens1 = tokenize_text(df1, max_len)\n",
    "\n",
    "# Model Creation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(tokens['input_ids'], df['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(tokens1['input_ids'], df1['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "train_inputs, train_labels = train_inputs1, train_labels1\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(train_inputs, train_labels, random_state=100, test_size=0.1)\n",
    "\n",
    "# Convert into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create DataLoader for the training, testing and validation set\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=32,shuffle  = True)\n",
    "val_data = TensorDataset(val_inputs, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)  # Add weight decay parameter\n",
    "\n",
    "best_acc = 0\n",
    "tolerance = 0\n",
    "# Keep track of the validation accuracy for each epoch\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(20):  # maximum of 100 epochs\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Apply weight decay\n",
    "        l2_regularization = torch.tensor(0., device=device)\n",
    "        for param in model.parameters():\n",
    "            l2_regularization += torch.norm(param, p=2)\n",
    "\n",
    "        loss += 0.01 * l2_regularization  # Adjust the weight decay factor\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # After each epoch, evaluate on validation set\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for batch in val_dataloader:\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        b_input_ids, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids)\n",
    "        logits = outputs[0].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "    # Check accuracy\n",
    "    acc = accuracy_score(true_labels, pred_flat)\n",
    "    val_acc_list.append(acc)\n",
    "\n",
    "    print(f'Validation accuracy: {acc}, epoch: {epoch}')\n",
    "\n",
    "    # If this epoch is better than before, reset patience\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        tolerance = 0\n",
    "    else:\n",
    "        # If this epoch did not improve accuracy, increase patience\n",
    "        tolerance += 1\n",
    "        if tolerance > 3:  # no improvement for 3 consecutive epochs\n",
    "            print('Early stopping due to no improvement')\n",
    "            break\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.plot(val_acc_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy vs. Number of Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = [b.to(device) for b in batch]\n",
    "    b_input_ids, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = outputs[0].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# For each input batch, pick the label (0 or 1) with the higher score\n",
    "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2fIFF6MaZrX"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
