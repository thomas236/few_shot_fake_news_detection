{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64fb204",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 250039,
     "status": "ok",
     "timestamp": 1692474745882,
     "user": {
      "displayName": "Thomas Sebastian",
      "userId": "15795710734502513346"
     },
     "user_tz": 240
    },
    "id": "b64fb204",
    "outputId": "f03918dd-2288-491d-943d-27a47553bf3f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if NLTK is installed, if not install it\n",
    "package = \"nltk\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Check if GloVe embeddings are present, if not download them\n",
    "if not os.path.isfile('./glove.6B.300d.txt'):\n",
    "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip glove*.zip\n",
    "\n",
    "def load_glove(path):\n",
    "    \"\"\"Loads GloVe embeddings.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "\n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "\n",
    "        for w in sentence_words:\n",
    "            if j >= max_len:\n",
    "                break\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                j = j + 1\n",
    "    return X_indices\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "word_to_index, index_to_word, word_to_vec_map = load_glove('./glove.6B.300d.txt')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/news_200.csv')\n",
    "df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/generated_combined_data200text-curie-001.csv\")\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "df = df.dropna(subset=['text', 'title'])\n",
    "df1 = df1.dropna(subset=['text', 'title'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_len = 256  # choose a max length\n",
    "tokens = sentences_to_indices(df['text'].values, word_to_index, max_len)\n",
    "tokens1 = sentences_to_indices(df1['text'].values, word_to_index, max_len)\n",
    "\n",
    "# Define the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.pretrained_embedding(word_to_vec_map, word_to_index, vocab_size, embedding_size))\n",
    "        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3)\n",
    "        self.fc = nn.Linear(256 * (max_len - 4), num_classes)\n",
    "\n",
    "    def pretrained_embedding(self, word_to_vec_map, word_to_index, vocab_size, embedding_size):\n",
    "        matrix_len = vocab_size\n",
    "        weights_matrix = np.zeros((matrix_len, embedding_size))\n",
    "        words_found = 0\n",
    "        for word, i in word_to_index.items():\n",
    "            try:\n",
    "                weights_matrix[i] = word_to_vec_map[word]\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size, ))\n",
    "        return torch.from_numpy(weights_matrix).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x.long())\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = len(word_to_index) + 1\n",
    "embedding_size = 300  # adjust the embedding size as needed\n",
    "num_classes = 2\n",
    "model = CNN(vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index)\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert labels into torch tensors\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    tokens, df['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(\n",
    "    tokens1, df1['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "train_inputs,train_labels=train_inputs1,train_labels1\n",
    "\n",
    "# Convert into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create DataLoader for the training set\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=32)\n",
    "\n",
    "# Training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids)\n",
    "        loss = criterion(outputs, b_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "# Create DataLoader for the test set\n",
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = [b.to(device) for b in batch]\n",
    "    b_input_ids, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = outputs.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# For each input batch, pick the label (0 or 1) with the higher score\n",
    "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CA_YEjAmUODK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241500,
     "status": "ok",
     "timestamp": 1692466481680,
     "user": {
      "displayName": "Thomas Sebastian",
      "userId": "15795710734502513346"
     },
     "user_tz": 240
    },
    "id": "CA_YEjAmUODK",
    "outputId": "b34e8bc3-7459-45cf-9742-33b95b032436"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if NLTK is installed, if not install it\n",
    "package = \"nltk\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Check if GloVe embeddings are present, if not download them\n",
    "if not os.path.isfile('./glove.6B.300d.txt'):\n",
    "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip glove*.zip\n",
    "\n",
    "def load_glove(path):\n",
    "    \"\"\"Loads GloVe embeddings.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "\n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "\n",
    "        for w in sentence_words:\n",
    "            if j >= max_len:\n",
    "                break\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                j = j + 1\n",
    "    return X_indices\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "word_to_index, index_to_word, word_to_vec_map = load_glove('./glove.6B.300d.txt')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/news_500.csv')\n",
    "df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/generated_combined_data500text-curie-001-3.csv\")\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "df = df.dropna(subset=['text', 'title'])\n",
    "df1 = df1.dropna(subset=['text', 'title'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_len = 256  # choose a max length\n",
    "tokens = sentences_to_indices(df['text'].values, word_to_index, max_len)\n",
    "tokens1 = sentences_to_indices(df1['text'].values, word_to_index, max_len)\n",
    "\n",
    "# Define the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.pretrained_embedding(word_to_vec_map, word_to_index, vocab_size, embedding_size))\n",
    "        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3)\n",
    "        self.fc = nn.Linear(256 * (max_len - 4), num_classes)\n",
    "\n",
    "    def pretrained_embedding(self, word_to_vec_map, word_to_index, vocab_size, embedding_size):\n",
    "        matrix_len = vocab_size\n",
    "        weights_matrix = np.zeros((matrix_len, embedding_size))\n",
    "        words_found = 0\n",
    "        for word, i in word_to_index.items():\n",
    "            try:\n",
    "                weights_matrix[i] = word_to_vec_map[word]\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size, ))\n",
    "        return torch.from_numpy(weights_matrix).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x.long())\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = len(word_to_index) + 1\n",
    "embedding_size = 300  # adjust the embedding size as needed\n",
    "num_classes = 2\n",
    "model = CNN(vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index)\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert labels into torch tensors\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    tokens, df['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(\n",
    "    tokens1, df1['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "train_inputs,train_labels=train_inputs1,train_labels1\n",
    "\n",
    "# Convert into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create DataLoader for the training set\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=32)\n",
    "\n",
    "# Training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids)\n",
    "        loss = criterion(outputs, b_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "# Create DataLoader for the test set\n",
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = [b.to(device) for b in batch]\n",
    "    b_input_ids, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = outputs.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# For each input batch, pick the label (0 or 1) with the higher score\n",
    "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PVFWJ6p4US63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32677,
     "status": "ok",
     "timestamp": 1692466514345,
     "user": {
      "displayName": "Thomas Sebastian",
      "userId": "15795710734502513346"
     },
     "user_tz": 240
    },
    "id": "PVFWJ6p4US63",
    "outputId": "4a5a3ed1-4dfb-4293-f322-df20fa327653"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if NLTK is installed, if not install it\n",
    "package = \"nltk\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Check if GloVe embeddings are present, if not download them\n",
    "if not os.path.isfile('./glove.6B.300d.txt'):\n",
    "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip glove*.zip\n",
    "\n",
    "def load_glove(path):\n",
    "    \"\"\"Loads GloVe embeddings.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "\n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "\n",
    "        for w in sentence_words:\n",
    "            if j >= max_len:\n",
    "                break\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                j = j + 1\n",
    "    return X_indices\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "word_to_index, index_to_word, word_to_vec_map = load_glove('./glove.6B.300d.txt')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/news_10000.csv')\n",
    "df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/generated_combined_data10000text-curie-001.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "df = df.dropna(subset=['text', 'title'])\n",
    "df1 = df1.dropna(subset=['text', 'title'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_len = 256  # choose a max length\n",
    "tokens = sentences_to_indices(df['text'].values, word_to_index, max_len)\n",
    "tokens1 = sentences_to_indices(df1['text'].values, word_to_index, max_len)\n",
    "\n",
    "# Define the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.pretrained_embedding(word_to_vec_map, word_to_index, vocab_size, embedding_size))\n",
    "        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3)\n",
    "        self.fc = nn.Linear(256 * (max_len - 4), num_classes)\n",
    "\n",
    "    def pretrained_embedding(self, word_to_vec_map, word_to_index, vocab_size, embedding_size):\n",
    "        matrix_len = vocab_size\n",
    "        weights_matrix = np.zeros((matrix_len, embedding_size))\n",
    "        words_found = 0\n",
    "        for word, i in word_to_index.items():\n",
    "            try:\n",
    "                weights_matrix[i] = word_to_vec_map[word]\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size, ))\n",
    "        return torch.from_numpy(weights_matrix).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x.long())\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = len(word_to_index) + 1\n",
    "embedding_size = 300  # adjust the embedding size as needed\n",
    "num_classes = 2\n",
    "model = CNN(vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index)\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert labels into torch tensors\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    tokens, df['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(\n",
    "    tokens1, df1['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "train_inputs,train_labels=train_inputs1,train_labels1\n",
    "\n",
    "# Convert into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create DataLoader for the training set\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=32)\n",
    "\n",
    "# Training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids)\n",
    "        loss = criterion(outputs, b_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "# Create DataLoader for the test set\n",
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = [b.to(device) for b in batch]\n",
    "    b_input_ids, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = outputs.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# For each input batch, pick the label (0 or 1) with the higher score\n",
    "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fzISvqZUfDR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35613,
     "status": "ok",
     "timestamp": 1692466549947,
     "user": {
      "displayName": "Thomas Sebastian",
      "userId": "15795710734502513346"
     },
     "user_tz": 240
    },
    "id": "2fzISvqZUfDR",
    "outputId": "02f74b9f-8557-41e9-c5fd-99c937ec80bd"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if NLTK is installed, if not install it\n",
    "package = \"nltk\"\n",
    "try:\n",
    "    importlib.import_module(package)\n",
    "except ImportError:\n",
    "    !pip install {package}\n",
    "    importlib.import_module(package)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Check if GloVe embeddings are present, if not download them\n",
    "if not os.path.isfile('./glove.6B.300d.txt'):\n",
    "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip glove*.zip\n",
    "\n",
    "def load_glove(path):\n",
    "    \"\"\"Loads GloVe embeddings.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "\n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "\n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "\n",
    "        for w in sentence_words:\n",
    "            if j >= max_len:\n",
    "                break\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                j = j + 1\n",
    "    return X_indices\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "word_to_index, index_to_word, word_to_vec_map = load_glove('./glove.6B.300d.txt')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/train.csv')\n",
    "df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/train_curie_20400.csv\")\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "df = df.dropna(subset=['text', 'title'])\n",
    "df1 = df1.dropna(subset=['text', 'title'])\n",
    "\n",
    "# Tokenize the text\n",
    "max_len = 256  # choose a max length\n",
    "tokens = sentences_to_indices(df['text'].values, word_to_index, max_len)\n",
    "tokens1 = sentences_to_indices(df1['text'].values, word_to_index, max_len)\n",
    "\n",
    "# Define the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.pretrained_embedding(word_to_vec_map, word_to_index, vocab_size, embedding_size))\n",
    "        self.conv1 = nn.Conv1d(embedding_size, 128, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3)\n",
    "        self.fc = nn.Linear(256 * (max_len - 4), num_classes)\n",
    "\n",
    "    def pretrained_embedding(self, word_to_vec_map, word_to_index, vocab_size, embedding_size):\n",
    "        matrix_len = vocab_size\n",
    "        weights_matrix = np.zeros((matrix_len, embedding_size))\n",
    "        words_found = 0\n",
    "        for word, i in word_to_index.items():\n",
    "            try:\n",
    "                weights_matrix[i] = word_to_vec_map[word]\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size, ))\n",
    "        return torch.from_numpy(weights_matrix).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x.long())\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = len(word_to_index) + 1\n",
    "embedding_size = 300  # adjust the embedding size as needed\n",
    "num_classes = 2\n",
    "model = CNN(vocab_size, embedding_size, num_classes, word_to_vec_map, word_to_index)\n",
    "model = model.to(device)\n",
    "\n",
    "# Convert labels into torch tensors\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    tokens, df['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(\n",
    "    tokens1, df1['label'].values, random_state=100, test_size=0.2)\n",
    "\n",
    "train_inputs,train_labels=train_inputs1,train_labels1\n",
    "\n",
    "# Convert into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create DataLoader for the training set\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=32)\n",
    "\n",
    "# Training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids)\n",
    "        loss = criterion(outputs, b_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "# Create DataLoader for the test set\n",
    "test_data = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = [b.to(device) for b in batch]\n",
    "    b_input_ids, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = outputs.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# For each input batch, pick the label (0 or 1) with the higher score\n",
    "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da13e4",
   "metadata": {
    "id": "32da13e4"
   },
   "source": [
    "# BERT Section Below"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1txr0rSCHH1cxKaqzxHP3Q2g2lyXcrNtO",
     "timestamp": 1690863605933
    },
    {
     "file_id": "1fu7oluUqVxR61mdFxiWL91-hXySJP_sT",
     "timestamp": 1690862466976
    },
    {
     "file_id": "1xovG9wwGJp1U2_a3wEk7ts4ie_x9fNqA",
     "timestamp": 1686508547571
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
