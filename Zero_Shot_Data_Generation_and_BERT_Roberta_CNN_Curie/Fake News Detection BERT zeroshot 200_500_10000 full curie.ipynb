{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b2a5802f0d6e4dc9b82360aa6c12a2d2","9abc4e2f955b475bb91efd7ef1ec7607","e7f94345ffdc45d0872a819808c5da5c","5af3d2a5276046cbaf8fb66db969f59f","92c99284382043cb9065a5c1febd15d4","b0acf75ac4fd44829a80158f7c2ac288","b1233d905de7463fb5939b9578e94769","45ebeb662a1a4977aebfd744c6abda9b","a93d8d14375849379a25d89150fa3642","448caa1c2c654d71bfd9d376ad5680f0","1c5f2dcadea04c5abcd78b4261bcd97d","181508deeea646ae8b32ac6025ea95f5","a3c83465bc0f48f4bab4fda6d74b2e57","261c1a5aa0dd40099b6001885aa05186","cb133379e4804e6893ed28232e1f18d4","6aa200d7093b4a688f1d92a12cb5c27f","e500f6eca2e24bb981e30bd1c05d4ede","1366ee831cd64231b30691a14cebe989","abfc63e9fc3c4d50890ac73cbe3b685c","72ededda77614290b0565f371f53c182","c49484b5384c46b8ad09efd16dd28203","040ea8c06cc344219ca3c4ba4d00a330","c32d4652dbd44c09a112fea42a5f9364","b4b7944c621b4870bae8616ed30677cf","e72ae597ed05433588ec4a9a4e4a5f04","20a1ae4eec3043feb1f378a956f4def0","5e31d7e7b09c4863b7bac4a844cfa416","0d96015182744d7380c94fb8f4bd90b0","4cf78c431a8447a4a0b5f95ffd653f25","36bdb0d0795e41bc968667b90591a7d6","25e38a17e8fe4bc58b691ae13cb833bd","32397a68f17d43da9e6793ade86e5c30","ee289d7581f1481c9303968366a9d72b","770a6694706c4d9ba0ea91f9f7b540df","451de0e2c0d740d38a21f9c3d1bb66ba","9e9321d25f7a49aa8b547b938a8512ac","728702faf06f4837a43d2143b6719e89","c5401426a34440a3845e070bc976c74c","d1287ad47543467a933c0801a0e158c4","a04352f98cd6424fa1296d6995419e14","d16d626014494c4ea31f2586f7d42eb9","4ec6619c934b4d189153adf56130fca5","53ba69f375da4f678262b0ff6ce43242","cd268c36656d4f53bf27c92df88baad1"]},"executionInfo":{"elapsed":70572,"status":"ok","timestamp":1692465247154,"user":{"displayName":"Thomas Sebastian","userId":"15795710734502513346"},"user_tz":240},"id":"b64fb204","outputId":"d3ccd60d-f3a8-40b9-da3d-253e4deef4a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=fba7727262e86b27ba84371da253062d17e5bbb35e7926184e6723796f4461c7\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Mounted at /content/drive\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2a5802f0d6e4dc9b82360aa6c12a2d2","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"181508deeea646ae8b32ac6025ea95f5","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c32d4652dbd44c09a112fea42a5f9364","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"770a6694706c4d9ba0ea91f9f7b540df","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","   Fake News       0.00      0.00      0.00        19\n","   Real News       0.51      1.00      0.68        20\n","\n","    accuracy                           0.51        39\n","   macro avg       0.26      0.50      0.34        39\n","weighted avg       0.26      0.51      0.35        39\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["import importlib\n","\n","package = \"tensorflow\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install - U {package}\n","    importlib.import_module(package)\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"pandas\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"sklearn\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"keras\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"torch\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"langdetect\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","\n","import re\n","from langdetect import detect\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/news_200.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/generated_combined_data200text-curie-001.csv\")\n","\n","\n","df = df.dropna(subset=['text'])\n","df = df.dropna(subset=['title'])\n","\n","df1 = df1.dropna(subset=['text'])\n","df1 = df1.dropna(subset=['title'])\n","\n","# Step 3: BERT Tokenization & Formatting\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","def tokenize_text(dfx, max_len):\n","    return tokenizer.batch_encode_plus(\n","        dfx['text'].tolist(),\n","        max_length = max_len,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        return_token_type_ids=False\n","    )\n","\n","# Tokenize the text\n","max_len = 256  # choose a max length\n","tokens = tokenize_text(df, max_len)\n","tokens1 = tokenize_text(df1, max_len)\n","\n","# Step 4: Model Creation\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=2,\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","\n","model = model.to(device)\n","\n","# Split into training and testing datasets\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(tokens['input_ids'], df['label'].values, random_state=100, test_size=0.2)\n","\n","# Split into training and testing datasets\n","train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(tokens1['input_ids'], df1['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs, train_labels = train_inputs1, train_labels1\n","\n","\n","# Convert into torch tensors\n","train_inputs = torch.tensor(train_inputs)\n","test_inputs = torch.tensor(test_inputs)\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","# Create DataLoader for the training set\n","train_data = TensorDataset(train_inputs, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=32)\n","\n","# Step 5: Training\n","#optimizer = AdamW(model.parameters(), lr=1e-5)\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5)  # Add weight decay parameter\n","\n","\n","for epoch in range(1):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)  # Add weight decay parameter\n","\n","for epoch in range(3):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","\n","        # Apply weight decay\n","        #l2_regularization = torch.tensor(0.)\n","        l2_regularization = torch.tensor(0., device=device)\n","        for param in model.parameters():\n","            l2_regularization += torch.norm(param, p=2)\n","\n","\n","        loss += 0.01 * l2_regularization  # Adjust the weight decay factor\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# Create DataLoader for the test set\n","test_data = TensorDataset(test_inputs, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=32)\n","\n","# Step 6: Evaluation\n","model.eval()\n","\n","predictions = []\n","true_labels = []\n","\n","for batch in test_dataloader:\n","    batch = [b.to(device) for b in batch]\n","    b_input_ids, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","\n","    # Move logits and labels to CPU\n","    logits = outputs[0].detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# For each input batch, pick the label (0 or 1) with the higher score\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","\n","# Print the classification report\n","print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"],"id":"b64fb204"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19633,"status":"ok","timestamp":1692465266772,"user":{"displayName":"Thomas Sebastian","userId":"15795710734502513346"},"user_tz":240},"id":"ovZFg29tOUYy","outputId":"d03436e2-3a7b-4942-e0df-42630a0d47a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","   Fake News       0.58      1.00      0.73        54\n","   Real News       1.00      0.13      0.24        45\n","\n","    accuracy                           0.61        99\n","   macro avg       0.79      0.57      0.48        99\n","weighted avg       0.77      0.61      0.51        99\n","\n"]}],"source":["import importlib\n","\n","package = \"tensorflow\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install - U {package}\n","    importlib.import_module(package)\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"pandas\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"sklearn\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"keras\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"torch\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"langdetect\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","\n","import re\n","from langdetect import detect\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/news_500.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/generated_combined_data500text-curie-001-3.csv\")\n","\n","df = df.dropna(subset=['text'])\n","df = df.dropna(subset=['title'])\n","\n","df1 = df1.dropna(subset=['text'])\n","df1 = df1.dropna(subset=['title'])\n","\n","# Step 3: BERT Tokenization & Formatting\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","def tokenize_text(dfx, max_len):\n","    return tokenizer.batch_encode_plus(\n","        dfx['text'].tolist(),\n","        max_length = max_len,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        return_token_type_ids=False\n","    )\n","\n","# Tokenize the text\n","max_len = 256  # choose a max length\n","tokens = tokenize_text(df, max_len)\n","tokens1 = tokenize_text(df1, max_len)\n","\n","# Step 4: Model Creation\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=2,\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","\n","model = model.to(device)\n","\n","# Split into training and testing datasets\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(tokens['input_ids'], df['label'].values, random_state=100, test_size=0.2)\n","\n","# Split into training and testing datasets\n","train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(tokens1['input_ids'], df1['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs, train_labels = train_inputs1, train_labels1\n","\n","\n","# Convert into torch tensors\n","train_inputs = torch.tensor(train_inputs)\n","test_inputs = torch.tensor(test_inputs)\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","# Create DataLoader for the training set\n","train_data = TensorDataset(train_inputs, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=32)\n","\n","# Step 5: Training\n","#optimizer = AdamW(model.parameters(), lr=1e-5)\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5)  # Add weight decay parameter\n","\n","\n","for epoch in range(1):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)  # Add weight decay parameter\n","\n","for epoch in range(3):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","\n","        # Apply weight decay\n","        #l2_regularization = torch.tensor(0.)\n","        l2_regularization = torch.tensor(0., device=device)\n","        for param in model.parameters():\n","            l2_regularization += torch.norm(param, p=2)\n","\n","\n","        loss += 0.01 * l2_regularization  # Adjust the weight decay factor\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# Create DataLoader for the test set\n","test_data = TensorDataset(test_inputs, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=32)\n","\n","# Step 6: Evaluation\n","model.eval()\n","\n","predictions = []\n","true_labels = []\n","\n","for batch in test_dataloader:\n","    batch = [b.to(device) for b in batch]\n","    b_input_ids, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","\n","    # Move logits and labels to CPU\n","    logits = outputs[0].detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# For each input batch, pick the label (0 or 1) with the higher score\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","\n","# Print the classification report\n","print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"],"id":"ovZFg29tOUYy"},{"cell_type":"code","execution_count":1,"metadata":{"id":"EvKan2NpOmPH","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4232d34df5a14b4d8c5bfd8875be4839","86a1d9b2dba94b53bb868a4ec65ff2c9","635eece9e8684c11aa67e3896768e68d","92168064e6f84a5a9b0972923856af9a","a216ca9f236d4e59baf7b4bc3cb6047a","1cf16fd627d64fcc9a2c169d3e79796f","43c212ff68224f00ada730eb2f92ae6a","2501e69943794fe3bf51e797f37aa1fa","b7e1b42a137e4c478efd680789b9be44","ec1c5ca93a3e42c58b2d42f9816f2bf1","1acaa515336e407ba9c82638e78982ca","e309d9fdd7b54d949669ba7cf75e005e","fdb2360ea82042dbb7fb6033fb766648","f73d37e0d9f042c9b833fbf9ad02bbab","d1d41ab54fc24a918c07cba3827301a5","08ded018866341b58618a1ede080635c","38ffe56e451f4bb59337830ff1395ca6","7b9508cb3110442189d9366fdf216486","f452a042d8a444a0b3f5edb9834962b6","0bfbac9c9ba24d9cb3f752e634e32216","05eee78328e1460193964888e765b68b","191dbbcdad1143cabc607d7a1fac0b12","da540def8c7b4b0db155cdd4a4a5fe11","8fa471fe88b04c1c8f47967085864749","d68a197b5d3443e99057974ce2e89baf","5142310e577c4b2c8b9866b5c82ed019","607d555f32f94abf906c90cab0ed072c","d9c7ea5bebc0490f9a04a778fc4aba8b","479c94c95fa14adc9bc5d93778e705a9","ec149f1e30a542cebd05eef2dbfe4d1b","4d21100d8efc4aafa349a490ce4438bb","65a5fd70393a43c09314e1a033b7d88c","9fc2b911cfa34081a3b22ac8c16515e1","7e02f20556da4e45a8c0bd9a8a5f681a","e1c492310949439d91e49cb7940a7944","19aea5ed9fae495ca39ff76435fc791c","3850ecf3ed2c466ab5292a2ce4ecc36d","db27403e02c94b54b739fa2fa5067881","d72066d5abf8451ebca9fd8850581992","90e0fa06e9434b9c88573d4cdc324260","65d1eeea4b0c49239ac4a60b60761d56","35ca420516194240b90c53055d0b1d56","86b7177ba8d64150b3d23a6d9a604aa5","26daf7718ad94f2ab66e2f6a67305782"]},"executionInfo":{"status":"ok","timestamp":1692474427876,"user_tz":240,"elapsed":518553,"user":{"displayName":"Thomas Sebastian","userId":"15795710734502513346"}},"outputId":"34840c45-af5b-41b4-cecb-fd79db27f005"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=0ca6bba282344e2279bba182c767f4066016b9b052e0d4613be7454c77fda9be\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Mounted at /content/drive\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4232d34df5a14b4d8c5bfd8875be4839"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e309d9fdd7b54d949669ba7cf75e005e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da540def8c7b4b0db155cdd4a4a5fe11"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e02f20556da4e45a8c0bd9a8a5f681a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","   Fake News       0.54      0.99      0.70      1029\n","   Real News       0.88      0.06      0.12       912\n","\n","    accuracy                           0.56      1941\n","   macro avg       0.71      0.53      0.41      1941\n","weighted avg       0.70      0.56      0.43      1941\n","\n"]}],"source":["import importlib\n","\n","package = \"tensorflow\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install - U {package}\n","    importlib.import_module(package)\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"pandas\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"sklearn\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"keras\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"torch\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"langdetect\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","\n","import re\n","from langdetect import detect\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/news_10000.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/generated_combined_data10000text-curie-001.csv\")\n","\n","\n","df = df.dropna(subset=['text'])\n","df = df.dropna(subset=['title'])\n","\n","df1 = df1.dropna(subset=['text'])\n","df1 = df1.dropna(subset=['title'])\n","\n","# Step 3: BERT Tokenization & Formatting\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","def tokenize_text(dfx, max_len):\n","    return tokenizer.batch_encode_plus(\n","        dfx['text'].tolist(),\n","        max_length = max_len,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        return_token_type_ids=False\n","    )\n","\n","# Tokenize the text\n","max_len = 256  # choose a max length\n","tokens = tokenize_text(df, max_len)\n","tokens1 = tokenize_text(df1, max_len)\n","\n","# Step 4: Model Creation\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=2,\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","\n","model = model.to(device)\n","\n","# Split into training and testing datasets\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(tokens['input_ids'], df['label'].values, random_state=100, test_size=0.2)\n","\n","# Split into training and testing datasets\n","train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(tokens1['input_ids'], df1['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs, train_labels = train_inputs1, train_labels1\n","\n","\n","# Convert into torch tensors\n","train_inputs = torch.tensor(train_inputs)\n","test_inputs = torch.tensor(test_inputs)\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","# Create DataLoader for the training set\n","train_data = TensorDataset(train_inputs, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=32)\n","\n","# Step 5: Training\n","#optimizer = AdamW(model.parameters(), lr=1e-5)\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5)  # Add weight decay parameter\n","\n","\n","for epoch in range(1):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)  # Add weight decay parameter\n","\n","for epoch in range(3):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","\n","        # Apply weight decay\n","        #l2_regularization = torch.tensor(0.)\n","        l2_regularization = torch.tensor(0., device=device)\n","        for param in model.parameters():\n","            l2_regularization += torch.norm(param, p=2)\n","\n","\n","        loss += 0.01 * l2_regularization  # Adjust the weight decay factor\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# Create DataLoader for the test set\n","test_data = TensorDataset(test_inputs, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=32)\n","\n","# Step 6: Evaluation\n","model.eval()\n","\n","predictions = []\n","true_labels = []\n","\n","for batch in test_dataloader:\n","    batch = [b.to(device) for b in batch]\n","    b_input_ids, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","\n","    # Move logits and labels to CPU\n","    logits = outputs[0].detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# For each input batch, pick the label (0 or 1) with the higher score\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","\n","# Print the classification report\n","print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"],"id":"EvKan2NpOmPH"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":312910,"status":"ok","timestamp":1692466680575,"user":{"displayName":"Thomas Sebastian","userId":"15795710734502513346"},"user_tz":240},"id":"YYJf1Z37OuKw","outputId":"b7f2e1d0-13f0-4b94-d34c-c0c2a0843f52"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","   Fake News       0.51      0.74      0.60      2104\n","   Real News       0.45      0.23      0.30      1937\n","\n","    accuracy                           0.49      4041\n","   macro avg       0.48      0.48      0.45      4041\n","weighted avg       0.48      0.49      0.46      4041\n","\n"]}],"source":["import importlib\n","\n","package = \"tensorflow\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install - U {package}\n","    importlib.import_module(package)\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"pandas\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"sklearn\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"keras\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"torch\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"transformers\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","package = \"langdetect\"\n","try:\n","    importlib.import_module(package)\n","except ImportError:\n","    !pip install {package}\n","    importlib.import_module(package)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import torch\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","\n","import re\n","from langdetect import detect\n","\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/train.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Zero_Shot_Data_Generation_and_BERT_Roberta_CNN_Curie/train_curie_20400.csv\")\n","\n","\n","df = df.dropna(subset=['text'])\n","df = df.dropna(subset=['title'])\n","\n","df1 = df1.dropna(subset=['text'])\n","df1 = df1.dropna(subset=['title'])\n","\n","# Step 3: BERT Tokenization & Formatting\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","def tokenize_text(dfx, max_len):\n","    return tokenizer.batch_encode_plus(\n","        dfx['text'].tolist(),\n","        max_length = max_len,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        return_token_type_ids=False\n","    )\n","\n","# Tokenize the text\n","max_len = 256  # choose a max length\n","tokens = tokenize_text(df, max_len)\n","tokens1 = tokenize_text(df1, max_len)\n","\n","# Step 4: Model Creation\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=2,\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","\n","model = model.to(device)\n","\n","# Split into training and testing datasets\n","train_inputs, test_inputs, train_labels, test_labels = train_test_split(tokens['input_ids'], df['label'].values, random_state=100, test_size=0.2)\n","\n","# Split into training and testing datasets\n","train_inputs1, test_inputs1, train_labels1, test_labels1 = train_test_split(tokens1['input_ids'], df1['label'].values, random_state=100, test_size=0.2)\n","\n","train_inputs, train_labels = train_inputs1, train_labels1\n","\n","\n","# Convert into torch tensors\n","train_inputs = torch.tensor(train_inputs)\n","test_inputs = torch.tensor(test_inputs)\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","# Create DataLoader for the training set\n","train_data = TensorDataset(train_inputs, train_labels)\n","train_dataloader = DataLoader(train_data, batch_size=32)\n","\n","# Step 5: Training\n","#optimizer = AdamW(model.parameters(), lr=1e-5)\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5)  # Add weight decay parameter\n","\n","\n","for epoch in range(1):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","\"\"\"\n","optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)  # Add weight decay parameter\n","\n","for epoch in range(3):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_labels = batch[1].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(b_input_ids, token_type_ids=None, labels=b_labels)\n","\n","        loss = outputs[0]\n","\n","        # Apply weight decay\n","        #l2_regularization = torch.tensor(0.)\n","        l2_regularization = torch.tensor(0., device=device)\n","        for param in model.parameters():\n","            l2_regularization += torch.norm(param, p=2)\n","\n","\n","        loss += 0.01 * l2_regularization  # Adjust the weight decay factor\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# Create DataLoader for the test set\n","test_data = TensorDataset(test_inputs, test_labels)\n","test_dataloader = DataLoader(test_data, batch_size=32)\n","\n","# Step 6: Evaluation\n","model.eval()\n","\n","predictions = []\n","true_labels = []\n","\n","for batch in test_dataloader:\n","    batch = [b.to(device) for b in batch]\n","    b_input_ids, b_labels = batch\n","\n","    with torch.no_grad():\n","        outputs = model(b_input_ids)\n","\n","    # Move logits and labels to CPU\n","    logits = outputs[0].detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Flatten the predictions and true values for aggregate evaluation on the whole dataset\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","\n","# For each input batch, pick the label (0 or 1) with the higher score\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","\n","# Print the classification report\n","print(classification_report(true_labels, pred_flat, target_names=['Fake News', 'Real News']))\n"],"id":"YYJf1Z37OuKw"}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"14LU86a2o2tDjFOp8l7d677nJ3wEjNNpt","timestamp":1690771167466},{"file_id":"1lYob1rPfGSf7FeXENk7RLqGyEy8uoHWE","timestamp":1690770955508},{"file_id":"1r20xJXSPGmjU6w71C2RIhMcnrp-VBYDR","timestamp":1690744032615},{"file_id":"1bdEsmb0UwC4fZ1zrRmjJb1RZKp1EYSy2","timestamp":1688930951314},{"file_id":"17C63qnZO6bvkANqwOyyPzEUNe0ZJluuL","timestamp":1688929870555},{"file_id":"1-B2oiTgXLFYEan4mLoHuEE2kBMXFkHv9","timestamp":1688928661031},{"file_id":"12Nnjz2ldFUJBms_l-pSmAwXkbX4G0mZE","timestamp":1688926225691},{"file_id":"1YTOxD9jEmm91uAbuAyg50qH5TNEUPXhX","timestamp":1688904820636},{"file_id":"1xovG9wwGJp1U2_a3wEk7ts4ie_x9fNqA","timestamp":1686508547571}],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"widgets":{"application/vnd.jupyter.widget-state+json":{"040ea8c06cc344219ca3c4ba4d00a330":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d96015182744d7380c94fb8f4bd90b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1366ee831cd64231b30691a14cebe989":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"181508deeea646ae8b32ac6025ea95f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3c83465bc0f48f4bab4fda6d74b2e57","IPY_MODEL_261c1a5aa0dd40099b6001885aa05186","IPY_MODEL_cb133379e4804e6893ed28232e1f18d4"],"layout":"IPY_MODEL_6aa200d7093b4a688f1d92a12cb5c27f"}},"1c5f2dcadea04c5abcd78b4261bcd97d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20a1ae4eec3043feb1f378a956f4def0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32397a68f17d43da9e6793ade86e5c30","placeholder":"​","style":"IPY_MODEL_ee289d7581f1481c9303968366a9d72b","value":" 570/570 [00:00&lt;00:00, 52.0kB/s]"}},"25e38a17e8fe4bc58b691ae13cb833bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"261c1a5aa0dd40099b6001885aa05186":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_abfc63e9fc3c4d50890ac73cbe3b685c","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_72ededda77614290b0565f371f53c182","value":28}},"32397a68f17d43da9e6793ade86e5c30":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36bdb0d0795e41bc968667b90591a7d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"448caa1c2c654d71bfd9d376ad5680f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"451de0e2c0d740d38a21f9c3d1bb66ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1287ad47543467a933c0801a0e158c4","placeholder":"​","style":"IPY_MODEL_a04352f98cd6424fa1296d6995419e14","value":"Downloading model.safetensors: 100%"}},"45ebeb662a1a4977aebfd744c6abda9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cf78c431a8447a4a0b5f95ffd653f25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ec6619c934b4d189153adf56130fca5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53ba69f375da4f678262b0ff6ce43242":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5af3d2a5276046cbaf8fb66db969f59f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_448caa1c2c654d71bfd9d376ad5680f0","placeholder":"​","style":"IPY_MODEL_1c5f2dcadea04c5abcd78b4261bcd97d","value":" 232k/232k [00:00&lt;00:00, 13.2MB/s]"}},"5e31d7e7b09c4863b7bac4a844cfa416":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aa200d7093b4a688f1d92a12cb5c27f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"728702faf06f4837a43d2143b6719e89":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53ba69f375da4f678262b0ff6ce43242","placeholder":"​","style":"IPY_MODEL_cd268c36656d4f53bf27c92df88baad1","value":" 440M/440M [00:00&lt;00:00, 500MB/s]"}},"72ededda77614290b0565f371f53c182":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"770a6694706c4d9ba0ea91f9f7b540df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_451de0e2c0d740d38a21f9c3d1bb66ba","IPY_MODEL_9e9321d25f7a49aa8b547b938a8512ac","IPY_MODEL_728702faf06f4837a43d2143b6719e89"],"layout":"IPY_MODEL_c5401426a34440a3845e070bc976c74c"}},"92c99284382043cb9065a5c1febd15d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9abc4e2f955b475bb91efd7ef1ec7607":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0acf75ac4fd44829a80158f7c2ac288","placeholder":"​","style":"IPY_MODEL_b1233d905de7463fb5939b9578e94769","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"9e9321d25f7a49aa8b547b938a8512ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d16d626014494c4ea31f2586f7d42eb9","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ec6619c934b4d189153adf56130fca5","value":440449768}},"a04352f98cd6424fa1296d6995419e14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3c83465bc0f48f4bab4fda6d74b2e57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e500f6eca2e24bb981e30bd1c05d4ede","placeholder":"​","style":"IPY_MODEL_1366ee831cd64231b30691a14cebe989","value":"Downloading (…)okenizer_config.json: 100%"}},"a93d8d14375849379a25d89150fa3642":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"abfc63e9fc3c4d50890ac73cbe3b685c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0acf75ac4fd44829a80158f7c2ac288":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1233d905de7463fb5939b9578e94769":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2a5802f0d6e4dc9b82360aa6c12a2d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9abc4e2f955b475bb91efd7ef1ec7607","IPY_MODEL_e7f94345ffdc45d0872a819808c5da5c","IPY_MODEL_5af3d2a5276046cbaf8fb66db969f59f"],"layout":"IPY_MODEL_92c99284382043cb9065a5c1febd15d4"}},"b4b7944c621b4870bae8616ed30677cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d96015182744d7380c94fb8f4bd90b0","placeholder":"​","style":"IPY_MODEL_4cf78c431a8447a4a0b5f95ffd653f25","value":"Downloading (…)lve/main/config.json: 100%"}},"c32d4652dbd44c09a112fea42a5f9364":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4b7944c621b4870bae8616ed30677cf","IPY_MODEL_e72ae597ed05433588ec4a9a4e4a5f04","IPY_MODEL_20a1ae4eec3043feb1f378a956f4def0"],"layout":"IPY_MODEL_5e31d7e7b09c4863b7bac4a844cfa416"}},"c49484b5384c46b8ad09efd16dd28203":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5401426a34440a3845e070bc976c74c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb133379e4804e6893ed28232e1f18d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c49484b5384c46b8ad09efd16dd28203","placeholder":"​","style":"IPY_MODEL_040ea8c06cc344219ca3c4ba4d00a330","value":" 28.0/28.0 [00:00&lt;00:00, 2.62kB/s]"}},"cd268c36656d4f53bf27c92df88baad1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1287ad47543467a933c0801a0e158c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d16d626014494c4ea31f2586f7d42eb9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e500f6eca2e24bb981e30bd1c05d4ede":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e72ae597ed05433588ec4a9a4e4a5f04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_36bdb0d0795e41bc968667b90591a7d6","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_25e38a17e8fe4bc58b691ae13cb833bd","value":570}},"e7f94345ffdc45d0872a819808c5da5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_45ebeb662a1a4977aebfd744c6abda9b","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a93d8d14375849379a25d89150fa3642","value":231508}},"ee289d7581f1481c9303968366a9d72b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4232d34df5a14b4d8c5bfd8875be4839":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86a1d9b2dba94b53bb868a4ec65ff2c9","IPY_MODEL_635eece9e8684c11aa67e3896768e68d","IPY_MODEL_92168064e6f84a5a9b0972923856af9a"],"layout":"IPY_MODEL_a216ca9f236d4e59baf7b4bc3cb6047a"}},"86a1d9b2dba94b53bb868a4ec65ff2c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cf16fd627d64fcc9a2c169d3e79796f","placeholder":"​","style":"IPY_MODEL_43c212ff68224f00ada730eb2f92ae6a","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"635eece9e8684c11aa67e3896768e68d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2501e69943794fe3bf51e797f37aa1fa","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b7e1b42a137e4c478efd680789b9be44","value":231508}},"92168064e6f84a5a9b0972923856af9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec1c5ca93a3e42c58b2d42f9816f2bf1","placeholder":"​","style":"IPY_MODEL_1acaa515336e407ba9c82638e78982ca","value":" 232k/232k [00:00&lt;00:00, 13.1MB/s]"}},"a216ca9f236d4e59baf7b4bc3cb6047a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cf16fd627d64fcc9a2c169d3e79796f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43c212ff68224f00ada730eb2f92ae6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2501e69943794fe3bf51e797f37aa1fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7e1b42a137e4c478efd680789b9be44":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec1c5ca93a3e42c58b2d42f9816f2bf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1acaa515336e407ba9c82638e78982ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e309d9fdd7b54d949669ba7cf75e005e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fdb2360ea82042dbb7fb6033fb766648","IPY_MODEL_f73d37e0d9f042c9b833fbf9ad02bbab","IPY_MODEL_d1d41ab54fc24a918c07cba3827301a5"],"layout":"IPY_MODEL_08ded018866341b58618a1ede080635c"}},"fdb2360ea82042dbb7fb6033fb766648":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38ffe56e451f4bb59337830ff1395ca6","placeholder":"​","style":"IPY_MODEL_7b9508cb3110442189d9366fdf216486","value":"Downloading (…)okenizer_config.json: 100%"}},"f73d37e0d9f042c9b833fbf9ad02bbab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f452a042d8a444a0b3f5edb9834962b6","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0bfbac9c9ba24d9cb3f752e634e32216","value":28}},"d1d41ab54fc24a918c07cba3827301a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05eee78328e1460193964888e765b68b","placeholder":"​","style":"IPY_MODEL_191dbbcdad1143cabc607d7a1fac0b12","value":" 28.0/28.0 [00:00&lt;00:00, 2.55kB/s]"}},"08ded018866341b58618a1ede080635c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38ffe56e451f4bb59337830ff1395ca6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b9508cb3110442189d9366fdf216486":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f452a042d8a444a0b3f5edb9834962b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bfbac9c9ba24d9cb3f752e634e32216":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"05eee78328e1460193964888e765b68b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"191dbbcdad1143cabc607d7a1fac0b12":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da540def8c7b4b0db155cdd4a4a5fe11":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8fa471fe88b04c1c8f47967085864749","IPY_MODEL_d68a197b5d3443e99057974ce2e89baf","IPY_MODEL_5142310e577c4b2c8b9866b5c82ed019"],"layout":"IPY_MODEL_607d555f32f94abf906c90cab0ed072c"}},"8fa471fe88b04c1c8f47967085864749":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9c7ea5bebc0490f9a04a778fc4aba8b","placeholder":"​","style":"IPY_MODEL_479c94c95fa14adc9bc5d93778e705a9","value":"Downloading (…)lve/main/config.json: 100%"}},"d68a197b5d3443e99057974ce2e89baf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec149f1e30a542cebd05eef2dbfe4d1b","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d21100d8efc4aafa349a490ce4438bb","value":570}},"5142310e577c4b2c8b9866b5c82ed019":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65a5fd70393a43c09314e1a033b7d88c","placeholder":"​","style":"IPY_MODEL_9fc2b911cfa34081a3b22ac8c16515e1","value":" 570/570 [00:00&lt;00:00, 52.5kB/s]"}},"607d555f32f94abf906c90cab0ed072c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9c7ea5bebc0490f9a04a778fc4aba8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"479c94c95fa14adc9bc5d93778e705a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec149f1e30a542cebd05eef2dbfe4d1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d21100d8efc4aafa349a490ce4438bb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65a5fd70393a43c09314e1a033b7d88c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fc2b911cfa34081a3b22ac8c16515e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e02f20556da4e45a8c0bd9a8a5f681a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1c492310949439d91e49cb7940a7944","IPY_MODEL_19aea5ed9fae495ca39ff76435fc791c","IPY_MODEL_3850ecf3ed2c466ab5292a2ce4ecc36d"],"layout":"IPY_MODEL_db27403e02c94b54b739fa2fa5067881"}},"e1c492310949439d91e49cb7940a7944":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d72066d5abf8451ebca9fd8850581992","placeholder":"​","style":"IPY_MODEL_90e0fa06e9434b9c88573d4cdc324260","value":"Downloading model.safetensors: 100%"}},"19aea5ed9fae495ca39ff76435fc791c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65d1eeea4b0c49239ac4a60b60761d56","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35ca420516194240b90c53055d0b1d56","value":440449768}},"3850ecf3ed2c466ab5292a2ce4ecc36d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86b7177ba8d64150b3d23a6d9a604aa5","placeholder":"​","style":"IPY_MODEL_26daf7718ad94f2ab66e2f6a67305782","value":" 440M/440M [00:00&lt;00:00, 576MB/s]"}},"db27403e02c94b54b739fa2fa5067881":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d72066d5abf8451ebca9fd8850581992":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90e0fa06e9434b9c88573d4cdc324260":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65d1eeea4b0c49239ac4a60b60761d56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35ca420516194240b90c53055d0b1d56":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86b7177ba8d64150b3d23a6d9a604aa5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26daf7718ad94f2ab66e2f6a67305782":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}